{"index":{"slug":"index","filePath":"index.md","title":"index","links":["readme"],"tags":[],"content":"readme"},"readme":{"slug":"readme","filePath":"readme.md","title":"readme","links":["RealAnalysis/readme","DifferentialEquation/readme","NumericalAnalysis/readme"],"tags":[],"content":"\nThis part of website are some random notes I made for some advance mathematics courses I took.\nNotes: The content can be incorrect, feel free to shoot me email.\n\nAuthor: ikuyo.dev/about\nReal Analysis Readme, Math 444\nDifferential Equation Readme, Math 441\nNumerical Analysis Readme, CS 450"},"DifferentialEquation/index":{"slug":"DifferentialEquation/index","filePath":"DifferentialEquation/index.md","title":"index","links":["DifferentialEquation/readme"],"tags":[],"content":"readme"},"DifferentialEquation/readme":{"slug":"DifferentialEquation/readme","filePath":"DifferentialEquation/readme.md","title":"readme","links":["DifferentialEquation/recap","DifferentialEquation/CH2/exact-equations","DifferentialEquation/CH2/integrating-factor","DifferentialEquation/CH3/fundamental-set","DifferentialEquation/CH3/Wronskian-determinant","DifferentialEquation/CH3/reduction-of-Order","DifferentialEquation/CH3/order-reduce-by-substitution","DifferentialEquation/CH3/repeated-root","DifferentialEquation/CH3/solution-difference","DifferentialEquation/CH3/variation-of-parameters","DifferentialEquation/CH4/linear-differential-operator","DifferentialEquation/CH4/characteristic-polynomial","DifferentialEquation/CH4/Abel's-identity","DifferentialEquation/CH5/radius-of-convergence","DifferentialEquation/CH5/series-solutions","NumericalAnalysis/readme","NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector","DifferentialEquation/CH7/systems-of-first-order-ODEs","NumericalAnalysis/CH5/Jacobian-matrix","DifferentialEquation/CH9/stability"],"tags":[],"content":"This note is outcome of SP 2025 MATH 441. Differential Equations in UIUC, taught by professor Nikolaos Tzirakis.\nThe text book is Elementary Differential Equations and Boundary Value Problems, Ninth Edition.\nTopic by chapter\nCH1 Introduction: recap\nCH2 First Order Differential Equations:\n\n2.6 exact equations and integrating factor technique\n\nCH3 Second Order Linear Equations:\n\nfundamental set\n3.2/3.3 Wronskian determinant\n3.4 reduction of Order technique, order reduce by substitution technique and repeated root\n3.5 solution difference technique\n3.6 variation of parameters technique\n\nCH4 Higher Order Linear Equations:\n\nlinear differential operator\ncharacteristic polynomial\n4.4 Abel’s identity\n\nCH5 Series Solutions of Second Order Linear Equations:\n\nradius of convergence\nseries solutions technique\n\nCH8 Numerical Methods: didn’t cover this chapter in class, but readme will cover some of it.\nCH7 Systems of First Order Linear Equations:\n\nEigenvalue and Eigenvector\nsystems of first order ODEs\n\nCH9 Nonlinear Differential Equations and Stability:\n\nJacobian matrix\nstability\n"},"DifferentialEquation/recap":{"slug":"DifferentialEquation/recap","filePath":"DifferentialEquation/recap.md","title":"recap","links":[],"tags":[],"content":"square root\n\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}"},"NumericalAnalysis/Recap":{"slug":"NumericalAnalysis/Recap","filePath":"NumericalAnalysis/Recap.md","title":"Recap","links":[],"tags":[],"content":"Determinant\n\\det(\n\\begin{bmatrix}\na &amp; b\\\\\nc &amp; d\n\\end{bmatrix}\n) = ad - cb\n\\det(\n\\begin{bmatrix}\na &amp; b &amp; c\\\\\nd &amp; e &amp; f \\\\\ng &amp; h &amp; i\n\\end{bmatrix}\n) = a(ei - fh) - b(di - fg) + c(dh - eg)\nEigenvalues\n\nNotation: \\lambda\n\nsolve \\det(A - \\lambda I) = 0\nSingular value\n\nNotation: \\delta\n\n\\delta^2 = \\lambda\nInverse of martixs\n(AB)^T = B^TA^T"},"NumericalAnalysis/index":{"slug":"NumericalAnalysis/index","filePath":"NumericalAnalysis/index.md","title":"index","links":["NumericalAnalysis/readme"],"tags":[],"content":"readme"},"NumericalAnalysis/matrix":{"slug":"NumericalAnalysis/matrix","filePath":"NumericalAnalysis/matrix.md","title":"matrix","links":["NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector","NumericalAnalysis/CH2/Orthogonal-matrix","NumericalAnalysis/CH2/Symmetric-positive-definite-matrix","NumericalAnalysis/CH2/Band-matrix","NumericalAnalysis/CH3/Vandermonde-matrix","NumericalAnalysis/CH3/Hessenberg-matrix","NumericalAnalysis/CH5/Jacobian-matrix","NumericalAnalysis/CH6/Hessian-matrix"],"tags":[],"content":"Operations of Matrix\nTranspose\nA^T and (AB)^T = B^TA^T\nConjugate transpose\nA^H\nDeterminant\n\\det(\n\\begin{bmatrix}\na &amp; b\\\\\nc &amp; d\n\\end{bmatrix}\n) = ad - cb\n\\det(\n\\begin{bmatrix}\na &amp; b &amp; c\\\\\nd &amp; e &amp; f \\\\\ng &amp; h &amp; i\n\\end{bmatrix}\n) = a(ei - fh) - b(di - fg) + c(dh - eg)\nProperties of Matrix\nRank\n\nThe rank of a matrix is equal to the number of non-zero rows if it is in Echelon Form.\n\nTypes of Matrix\n\nSingular, \\det(A) = 0. It’s inverse is not exists.\nDiagonal, (\\forall i,j) i \\neq  j \\Longrightarrow a_{ij} = 0.\nTridiagonal, (\\forall i,j)|i - j| &gt; 1 \\Longrightarrow a_{ij} = 0.\nTriangular, lower: (\\forall i,j)i &gt; j \\Longrightarrow a_{ij} = 0; higher: (\\forall i,j)i &lt; j \\Longrightarrow a_{ij} = 0.\nSymmetric, A=A^T.\nSkew-Symmetric, A=-A^T.\nHermitian, A = A^H.\nUnitary, A^HA = AA^H = I.\nNormal A=A^H; A^HA=AA^H.\nDefective, Multiplicity k &gt; 1 with fewer than k linearly independent corresponding Eigenvectors.\nNondefective / diagonalizable, it has X^{-1}AX = D where X is Eigenvectors.\n\n\n\nOrthogonal matrix, A^TA=AA^T=I.\nSymmetric positive definite matrix (SPD)\nBand matrix\nVandermonde matrix\nHessenberg matrix\nJacobian matrix\nHessian matrix\n"},"NumericalAnalysis/readme":{"slug":"NumericalAnalysis/readme","filePath":"NumericalAnalysis/readme.md","title":"readme","links":["NumericalAnalysis/Recap","NumericalAnalysis/matrix","NumericalAnalysis/CH2/LU-factorization","NumericalAnalysis/CH2/Cholesky-factorization","NumericalAnalysis/CH3/QR-factorization","NumericalAnalysis/CH3/Singular-value-decomposition","NumericalAnalysis/CH2/Forward-backward-substitution","NumericalAnalysis/CH3/normal-equation","NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector","NumericalAnalysis/CH4/power-iteration","NumericalAnalysis/CH4/inverse-iteration","NumericalAnalysis/CH4/Rayleigh-quotient-iteration","NumericalAnalysis/CH4/Rayleigh-quotient","NumericalAnalysis/CH4/deflation","NumericalAnalysis/CH4/orthogonal-iteration","NumericalAnalysis/CH4/QR-iteration","NumericalAnalysis/CH5/convergence-rate","NumericalAnalysis/CH5/multiplicity","NumericalAnalysis/CH5/bisection-method","NumericalAnalysis/CH5/Aitken's-method","NumericalAnalysis/CH5/Newton's-method","NumericalAnalysis/CH5/Secant's-method","NumericalAnalysis/CH5/Muller's-method","NumericalAnalysis/CH5/Newton's-method---multi","NumericalAnalysis/CH5/Broyden’s-Method","NumericalAnalysis/CH5/inverse-quadratic-interpolation","NumericalAnalysis/CH6/coercive","NumericalAnalysis/CH6/convex","NumericalAnalysis/CH6/level-set","NumericalAnalysis/CH6/unimodal","NumericalAnalysis/CH6/critical-points","NumericalAnalysis/CH6/gradient","NumericalAnalysis/CH6/Hessian-matrix","NumericalAnalysis/CH6/single/Golden-Section-Search","NumericalAnalysis/CH6/single/Newton’s-method","NumericalAnalysis/CH6/single/Successive-Parabolic-Interpolation","NumericalAnalysis/CH6/multi/Newton's-method","NumericalAnalysis/CH6/multi/BFGS-Method","NumericalAnalysis/CH6/multi/Steepest-Descent-Method","NumericalAnalysis/CH6/multi/Conjugate-Gradient-Method","NumericalAnalysis/CH6/multi/Gauss-Newton-Method","NumericalAnalysis/CH6/multi/Levenberg-Marquardt-method","NumericalAnalysis/CH6/multi/Nonlinear-Least-Squares","NumericalAnalysis/CH6/Lagrangian-function","NumericalAnalysis/CH7/basis-function","NumericalAnalysis/CH7/Lagrange-Interpolation","NumericalAnalysis/CH7/orthogonal-polynomial-interpolation","NumericalAnalysis/CH7/Legendre-Polynomials","NumericalAnalysis/CH7/Chebyshev-Polynomials","NumericalAnalysis/CH7/Chebyshev-interpolation","NumericalAnalysis/CH7/Splines","NumericalAnalysis/CH8/Richardson-Extrapolation","NumericalAnalysis/CH8/basic-difference","NumericalAnalysis/CH8/Quadrature","NumericalAnalysis/CH8/Newton-Cotes-Quadrature","NumericalAnalysis/CH8/Gaussian-Quadrature","NumericalAnalysis/CH9/Forward-Euler","NumericalAnalysis/CH9/Backward-Euler","NumericalAnalysis/CH9/Trapezoidal-Rule","NumericalAnalysis/CH9/Runge-Kutta-method","NumericalAnalysis/CH1/","NumericalAnalysis/CH2/","NumericalAnalysis/CH3/","NumericalAnalysis/CH4/","NumericalAnalysis/CH5/","NumericalAnalysis/CH6/","NumericalAnalysis/CH7/","NumericalAnalysis/CH8/","NumericalAnalysis/CH9/"],"tags":[],"content":"This note is outcome of Spring 25 CS 450. Numerical Analysis in UIUC taught by instructor Paul Fischer.\nThe book is Scientific Computing: An Introductory Survey, Revised Second Edition.\nTopics by type\nRecap\nTypes of Matrix\nFactorization:\n\nLU factorization\n\nCholesky factorization w/ SPD\n\n\nQR factorization based on orthogonal\nSingular value decomposition (SVD)\n\nSolving:\n\nForward-backward substitution w/ LU factorization\nnormal equation w/ Vandermonde matrix to solve polynomial\n\nEigenvalue and Eigenvector problem:\n\n\nEigenvalue and Eigenvector\n\n\nFind Eigenvectors by iterations:\n\npower iteration and it’s variants normalized power iteration and power iteration with shift\ninverse iteration\nRayleigh quotient iteration and Rayleigh quotient\ndeflation\n\n--- multi eign-solver ---\n\northogonal iteration and QR iteration\n\n\n\nNonlinear equations:\n\nconvergence rate: how fast is a method finding solutions\nmultiplicity\nbisection method\nAitken’s method\nNewton’s method and it’s variants Secant’s method, Muller’s method\nNewton’s method - multi for system of nonlinear equations and it’s variants Broyden’s Method\ninverse quadratic interpolation\n\nOptimization:\n\nsome terms: coercive, convex, level set, unimodal, critical points, gradient, Hessian matrix\nGolden Section Search, Newton’s method, Successive Parabolic Interpolation for single variable (1-D)\nNewton’s method, BFGS Method, Steepest Descent Method, Conjugate Gradient Method for system of variables(n-D)\nGauss Newton Method, Levenberg-Marquardt method and Nonlinear Least Squares\nConstrained Optimality: Lagrangian function\n\nPolynomial Interpolation:\n\nthere are mainly two types:\n\nglobal, like using basis function, Lagrange Interpolation or orthogonal polynomial interpolation (with Legendre Polynomials or Chebyshev Polynomials), Chebyshev interpolation (basically interpolation used Chebyshev nodes)\npiece-wise linear or piece-wise polynomial (more stable than global) like Splines\n\n\nThe error analysis for global interpolation: interpolation error\n\nNumerical Integration and Differentiation:\n\nRichardson Extrapolation\nbasic difference\nQuadrature, Newton-Cotes Quadrature and Gaussian Quadrature\n\nOrdinary Differential Equations:\n\nForward Euler and Backward Euler\nTrapezoidal Rule and Runge-Kutta method\n\nTopics by Chapter\nChapter 1 Introduction\nChapter 2 System of linear equations\nChapter 3 Linear Least Squares\nChapter 4 Eigenvalue problems\nChapter 5 Nonlinear equations\nChapter 6 Optimization\nChapter 7 Interpolation\nChapter 8 Numerical Integration and Differentiation\nChapter 9 Ordinary Differential Equations"},"RealAnalysis/index":{"slug":"RealAnalysis/index","filePath":"RealAnalysis/index.md","title":"index","links":["RealAnalysis/readme"],"tags":[],"content":"readme"},"RealAnalysis/readme":{"slug":"RealAnalysis/readme","filePath":"RealAnalysis/readme.md","title":"readme","links":["RealAnalysis/sequences/Cauchy-sequence","RealAnalysis/sequences/complete","RealAnalysis/sequences/closed","RealAnalysis/sequences/compact","RealAnalysis/sequences/connected","RealAnalysis/sequences/sequentially-Closed","RealAnalysis/sequences/sequentially-compact","RealAnalysis/sequences/continue","RealAnalysis/sequences/convergent","RealAnalysis/sequences/sequentially-continue","RealAnalysis/sequences/Lipschitz","RealAnalysis/sequences/bounded","RealAnalysis/sequences/totally-bounded","RealAnalysis/sequences/Delta-ball","RealAnalysis/sequences/Metric-Space","RealAnalysis/integrableAndDiff/integrable","RealAnalysis/integrableAndDiff/integral","RealAnalysis/integrableAndDiff/Darboux-integral","RealAnalysis/integrableAndDiff/Riemann-integral","RealAnalysis/integrableAndDiff/differentiable","RealAnalysis/integrableAndDiff/mesh","RealAnalysis/integrableAndDiff/partition","RealAnalysis/integrableAndDiff/proofs/Proofs-of-pi","RealAnalysis/sequences/proofs/3-lemmas-of-class-28","RealAnalysis/sequences/proofs/proof-of-Thm-21.7","RealAnalysis/sequences/proofs/proof-of-Thm-15.1","RealAnalysis/integrableAndDiff/proofs/Final-Review-problems"],"tags":[],"content":"This note is outcome of Fall 24 Math 444. Elementary Real Analysis in UIUC, taught by professor Marius Junge.\nThe number after sub-heading indicates the number of class(e.g. Def 37.3 means 3rd point in lecture #37) and the book is Introduction to Real Analysis 4th Edition by Robert G. Bartle.\nTopics\nKey points for sequences:\n\nCauchy sequence\nCs for sets: complete, closed, compact, connected\nsequential version: sequentially Closed, sequentially compact\nC for functions: continue, convergent\nsequential version: sequentially continue\nuseful tools: Lipschitz, bounded and totally bounded, Delta-ball, Metric Space\n\nKey points for integrable:\n\nintegrable, integral\nDarboux integral and Riemann integral\ndifferentiable\nmesh and partition\n\nSome proofs:\n\n\\pi &gt;= 2 &gt;= 2\\sqrt{2} --- Proofs of pi\nSet S is totally bounded \\Longrightarrow S is bounded ---  3 lemmas of class 28\nSet S is compact \\Longrightarrow S is bounded set --- proof of Thm 21.7\nFor Metric Space X(C,d), C is closed \\Longrightarrow X is complete --- proof of Thm 15.1\nRiemann integral of \\lim_{a\\longrightarrow 1} \\sum cos(a^\\frac{j}{2})a^j(1-a) --- Final Review problems\n"},"DifferentialEquation/CH2/exact-equations":{"slug":"DifferentialEquation/CH2/exact-equations","filePath":"DifferentialEquation/CH2/exact equations.md","title":"exact equations","links":[],"tags":[],"content":"For a differential equation\nM(x,y)dx + N(x,y)dy = 0\nwe will say it’s exact if and only if\n\\frac{\\partial M}{dy} = \\frac{\\partial N}{dx}"},"DifferentialEquation/CH2/integrating-factor":{"slug":"DifferentialEquation/CH2/integrating-factor","filePath":"DifferentialEquation/CH2/integrating factor.md","title":"integrating factor","links":[],"tags":[],"content":"For\n\\frac{dy}{dt} + p(t)y(t) = g(t)\nconsider a \\mu(t) that\n\\mu\\frac{dy}{dt} + \\mu p(t)y(t) = \\mu g(t)\ntake \\frac{d\\mu}{dt} = \\mu p(t) \\implies \\mu = Ce^{p(t)t}\ntherefore, we can turn the equation above to\n\\frac{d}{dt}(\\mu y) = \\mu g(t) \\implies y = \\frac{\\int \\mu g(t) dt + C}{\\mu}\nthen plug \\mu in as the solution."},"DifferentialEquation/CH3/Wronskian-determinant":{"slug":"DifferentialEquation/CH3/Wronskian-determinant","filePath":"DifferentialEquation/CH3/Wronskian determinant.md","title":"Wronskian determinant","links":[],"tags":[],"content":"also just called `Wronskian\nW(y_1, y_2, \\dots, y_n)(x) =\n\\begin{vmatrix}\ny_1(x) &amp; y_2(x) &amp; \\cdots &amp; y_n(x) \\\\\ny_1&#039;(x) &amp; y_2&#039;(x) &amp; \\cdots &amp; y_n&#039;(x) \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\ny_1^{(n-1)}(x) &amp; y_2^{(n-1)}(x) &amp; \\cdots &amp; y_n^{(n-1)}(x)\n\\end{vmatrix}\nfor example if n=2\nW(y_1, y_2) = \\begin{vmatrix}\n        y_1 &amp; y_2\\\\\n        y_1&#039; &amp; y_2&#039;\n    \\end{vmatrix}\nthen y_1, \\dots y_n are linearly independent if and only if W \\neq 0.\nThis is important because in solving a linear system of differential equations, we need a fundamental set of linearly independent solutions. This is analogous to solving a system of linear equations, where the system has a unique solution only if the coefficient matrix is invertible—that is, its determinant is nonzero."},"DifferentialEquation/CH3/fundamental-set":{"slug":"DifferentialEquation/CH3/fundamental-set","filePath":"DifferentialEquation/CH3/fundamental set.md","title":"fundamental set","links":[],"tags":[],"content":"a set of solutions for the specific differential equation\nevery solution should be linear combination of this"},"DifferentialEquation/CH3/order-reduce-by-substitution":{"slug":"DifferentialEquation/CH3/order-reduce-by-substitution","filePath":"DifferentialEquation/CH3/order reduce by substitution.md","title":"order reduce by substitution","links":[],"tags":[],"content":"a technique to solve higher order differential equation by setting additional variable to substitute. Then solve it by normal ODE method. Lastly substitute it back\nExample\n(2-t)u&#039;&#039;&#039; + (3-t)u&#039;&#039; = 0\nwhere t &lt; 2\nconsider z = u&#039;&#039;\n\\begin{equation*}\\begin{aligned}\n    (2-t)z&#039; + (3-t)z &amp;= 0\\\\\n    z&#039; &amp;= \\frac{t-3}{2 - t}z\\\\\n    z&#039;\\frac{1}{z} &amp;= \\frac{t-3}{2 - t}\\\\\n    z &amp;= Ce^{\\int \\frac{t-3}{2 - t} dt}\\\\\n    z &amp;= C(t-2)e^{-t}\n\\end{aligned}\\end{equation*}\nthen\n\\begin{equation*}\\begin{aligned}\n    u&#039;&#039; &amp;= C(t-2)e^{-t}\\\\\n    u&#039; &amp;= -C_1e^{-t}(t -1) + C_2\\\\\n    u &amp;= -C_1e^{-t}t + C_3 + C_2t\n\\end{aligned}\\end{equation*}\n\\square"},"DifferentialEquation/CH3/reduction-of-Order":{"slug":"DifferentialEquation/CH3/reduction-of-Order","filePath":"DifferentialEquation/CH3/reduction of Order.md","title":"reduction of Order","links":[],"tags":[],"content":"find another linearly independent solution based on a known solution\nknown y_1(t) as solution, take y_2(t) = u(t)y_1(t) as new solution"},"DifferentialEquation/CH3/repeated-root":{"slug":"DifferentialEquation/CH3/repeated-root","filePath":"DifferentialEquation/CH3/repeated root.md","title":"repeated root","links":["NumericalAnalysis/CH5/multiplicity"],"tags":[],"content":"which is a root with has algebraic multiplicity of &gt;1.\nfor root with multiplicity of m\n(\\sum_{i=0}^m C_it^i)e^{rt}\nfor example (C_1t + C_2)e^{rt} if multiplicity of 2."},"DifferentialEquation/CH3/solution-difference":{"slug":"DifferentialEquation/CH3/solution-difference","filePath":"DifferentialEquation/CH3/solution difference.md","title":"solution difference","links":[],"tags":[],"content":"a technique in solving, theorem 3.5.1 in the book\n"},"DifferentialEquation/CH3/variation-of-parameters":{"slug":"DifferentialEquation/CH3/variation-of-parameters","filePath":"DifferentialEquation/CH3/variation of parameters.md","title":"variation of parameters","links":[],"tags":[],"content":"\\begin{equation*}\\begin{aligned}\ny&#039; + p(t)y &amp;= 0\\\\\ny&#039;\\frac{1}{y} &amp;= - p(t)\\\\\n(\\ln|y|)&#039; &amp;= -p(t)\\\\\ny &amp;= Ce^{-\\int p(t) dt}\n\\end{aligned}\\end{equation*}"},"DifferentialEquation/CH4/Abel's-identity":{"slug":"DifferentialEquation/CH4/Abel's-identity","filePath":"DifferentialEquation/CH4/Abel's identity.md","title":"Abel's identity","links":["DifferentialEquation/CH3/Wronskian-determinant"],"tags":[],"content":"for\ny&#039;&#039; + p(t) y&#039; + q(t) y = 0\nW(t) = W(y_1, \\dots, y_n)(t) = c e^{-\\int p(t)dt}\nwhere W is Wronskian determinant and c is a constant by W at convenient point."},"DifferentialEquation/CH4/characteristic-polynomial":{"slug":"DifferentialEquation/CH4/characteristic-polynomial","filePath":"DifferentialEquation/CH4/characteristic polynomial.md","title":"characteristic polynomial","links":["DifferentialEquation/CH4/characteristic-polynomial"],"tags":[],"content":"in most cases solving the equation, we can extract a non-zero common factor, e.g. e^rt, then the result of the part is called characteristic polynomial with notation z(r), and our goal is solving z(r) = 0"},"DifferentialEquation/CH4/linear-differential-operator":{"slug":"DifferentialEquation/CH4/linear-differential-operator","filePath":"DifferentialEquation/CH4/linear differential operator.md","title":"linear differential operator","links":[],"tags":[],"content":"a alias to the differential equation, for example these two situation are equivalent:\ny&#039;&#039; - y&#039; = 0 then consider y = e^rt to solve it\nand\nL[y] = 0 where L = y&#039;&#039; - y&#039; consider solve L[e^{rt}] = 0"},"DifferentialEquation/CH5/radius-of-convergence":{"slug":"DifferentialEquation/CH5/radius-of-convergence","filePath":"DifferentialEquation/CH5/radius of convergence.md","title":"radius of convergence","links":["DifferentialEquation/CH5/radius-of-convergence"],"tags":[],"content":"for power series\n\\sum^\\infty_na_n\nlike the content in Calculus II, do the ratio test\n\\lim_{n\\to\\infty}|\\frac{a_{n+1}}{a_n}|\nthen the radius of convergence is \\frac{1}{N} where N is at most what x can let it less than 1 (convergence condition).\nExample\n\\sum^\\infty_{n = 0} 2^nx^n\nSo\na_n = 2^nx^n\nand\na_{n + 1} = 2x2^nx^n\nthen\n\\lim_{n \\to \\infty} |\\frac{a_{n + 1}}{a_n}| = |2x|\nThen we can say the radius of convergence is R = \\frac{1}{N} = \\frac{1}{2}.  \\square"},"DifferentialEquation/CH5/series-solutions":{"slug":"DifferentialEquation/CH5/series-solutions","filePath":"DifferentialEquation/CH5/series solutions.md","title":"series solutions","links":[],"tags":[],"content":"consider\ny = \\sum^\\infty_{n = 0} a_nx^n\nthen\ny&#039; = \\sum^\\infty_{n = 1} na_nx^{n - 1} = \\sum^\\infty_{n = 0} (n+1)a_{n+1}x^n\nand\ny&#039;&#039; = \\sum^\\infty_{n = 2} n(n-1)a_nx^{n - 2} =  \\sum^\\infty_{n = 0} (n + 2)(n + 1)a_{n + 2}x^n\nthen solve the equation by normal calculation"},"DifferentialEquation/CH7/systems-of-first-order-ODEs":{"slug":"DifferentialEquation/CH7/systems-of-first-order-ODEs","filePath":"DifferentialEquation/CH7/systems of first order ODEs.md","title":"systems of first order ODEs","links":["NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector","DifferentialEquation/CH3/repeated-root"],"tags":[],"content":"\\vec x&#039; = A\\vec x\nbriefly we need to find all Eigenvectors:\n\nfind all Eigenvalues of the coefficient matrix A by solving \\det(A - \\lambda I)\nfor each \\lambda_i find (A - \\lambda_i I)\\vec \\xi_i = \\vec 0 where I is identify matrix and \\vec\\xi_i is correspond Eigenvector\nif the \\lambda_i is repeated eigenvalue (equivalent to repeated root), need to use (A - \\lambda_i I)\\vec \\xi_{i+1} = \\vec \\xi_i to find the generalized eigenvector, this process also called Jordan Chain\nlastly construct the homogeneous solution\n\\vec x(t) = \\sum C_i \\xi_ie^{\\lambda_i t}\n\n"},"DifferentialEquation/CH9/stability":{"slug":"DifferentialEquation/CH9/stability","filePath":"DifferentialEquation/CH9/stability.md","title":"stability","links":["NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector"],"tags":[],"content":"\nwhere r_1,r_2 are Eigenvalue"},"NumericalAnalysis/CH1/index":{"slug":"NumericalAnalysis/CH1/index","filePath":"NumericalAnalysis/CH1/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH2/Band-matrix":{"slug":"NumericalAnalysis/CH2/Band-matrix","filePath":"NumericalAnalysis/CH2/Band matrix.md","title":"Band matrix","links":["NumericalAnalysis/CH2/LU-factorization","NumericalAnalysis/CH2/Band-matrix"],"tags":[],"content":"A is m\\times n\nDefinition\n(\\forall |j - i| &gt; b)a_{ij} = 0\nCosts\nStorage/solve cost is \\approx 2nb\nLU factorization cost for Band matrix is \\approx 2nb^2 operations"},"NumericalAnalysis/CH2/Cholesky-factorization":{"slug":"NumericalAnalysis/CH2/Cholesky-factorization","filePath":"NumericalAnalysis/CH2/Cholesky factorization.md","title":"Cholesky factorization","links":["NumericalAnalysis/CH2/LU-factorization","NumericalAnalysis/CH2/Symmetric-positive-definite-matrix"],"tags":[],"content":"This is a special case for LU factorization (U = L^T).\nA=LL^T\nwhere\n\nA m\\times n is SPD\n\n2x2 example\nL = \\begin{bmatrix} \nx &amp; 0 \\\\\ny &amp; z\n\\end{bmatrix}\nL^T = \\begin{bmatrix}\nx &amp; y \\\\\n0 &amp; z\n\\end{bmatrix}\nso\nA = LL^T = \\begin{bmatrix}\nx^2 &amp; xy \\\\\nxy &amp; y^2 + z^2\n\\end{bmatrix}\n\\square\nCosts\nOnly lower triangular part of A is accessed, so only \\frac{1}{2} the storage is required.\nOnly \\frac{n^3}{6} multiplications and additions required, so \\frac{1}{2} the work of LU factorization\nNo need to pivoting"},"NumericalAnalysis/CH2/Forward-backward-substitution":{"slug":"NumericalAnalysis/CH2/Forward-backward-substitution","filePath":"NumericalAnalysis/CH2/Forward-backward substitution.md","title":"Forward-backward substitution","links":["NumericalAnalysis/CH2/LU-factorization"],"tags":[],"content":"\nref: ubcmath.github.io/MATH307/systems/lu.html#forward-and-backward-substitution\n\nGiven A=LU by LU factorization, we can adapt this technique to solve Ax= b by Ly=b and Ux = y\nPython code example for LU factorization\nnp.linalg\nit is equivalent with codes:\ny = np.linalg.solve(L, b)\nx = np.linalg.solve(U, y)\ncustom implementation\nimport numpy as np\n \ndef fw_sub(L, b):\n    n = L.shape[0]\n    y = np.zeros((n, 1))\n    \n    for i in range(n):\n        sum_val = 0\n        for j in range(i):\n            sum_val += L[i, j] * y[j, 0]\n        y[i, 0] = (b[i, 0] - sum_val) / L[i, i]\n    \n    return y\n \ndef bw_sub(U, y):\n    n = U.shape[0]\n    x = np.zeros((n, 1))\n    \n    for i in range(n-1, -1, -1):\n        sum_val = 0\n        for j in range(i+1, n):\n            sum_val += U[i, j] * x[j, 0]\n        x[i, 0] = (y[i, 0] - sum_val) / U[i, i]\n    \n    return x\n \nL = np.array([\n\t[1, 0, 0],\n\t[2, 1, 0],\n\t[1, 1, 1]\n])\nU = np.array([\n\t[2, 4, 2],\n\t[0, 1, 1],\n\t[0, 0, 1]\n])\n \n# solve Ax = b\nA = L @ U\nb = np.array([[-1], [1], [2]])\nprint(f&quot;L={L}&quot;)\nprint(f&quot;U={U}&quot;)\nprint(f&quot;b={b}&quot;)\n# A=[[2 4 2]\n# [4 9 5]\n# [2 5 4]]\nprint(f&quot;A={A}&quot;)\n \n# solve Ly = b\ny = fw_sub(L, b)\n# solve Ux = y\nx = bw_sub(U, y)\n \n# y=[[-1.]\n#  [ 3.]\n#  [ 0.]]\nprint(f&quot;y={y}&quot;)\n# x=[[-6.5]\n#  [ 3. ]\n#  [ 0. ]]\nprint(f&quot;x={x}&quot;)\n# Ax=[[-1.]\n#  [ 1.]\n#  [ 2.]]\n# Ax should be eq to b\nprint(f&quot;Ax={A @ x}&quot;)\n "},"NumericalAnalysis/CH2/LU-factorization":{"slug":"NumericalAnalysis/CH2/LU-factorization","filePath":"NumericalAnalysis/CH2/LU factorization.md","title":"LU factorization","links":["NumericalAnalysis/CH2/Symmetric-positive-definite-matrix","NumericalAnalysis/CH2/Cholesky-factorization","NumericalAnalysis/CH2/Forward-backward-substitution"],"tags":[],"content":"factorizing by A = LU where L is lower triangular and U is upper triangular matrix.\nBasic factorization method\nMake all diagonal elements of L to one and manually calculate it with U.\n3x3 example\nL =\n\\begin{bmatrix}\n1 &amp; 0 &amp; 0\\\\\nx &amp; 1 &amp; 0 \\\\\ny &amp; z &amp; 1\n\\end{bmatrix}\nU =\n\\begin{bmatrix}\na &amp; b &amp; c \\\\\n0 &amp; d &amp; e \\\\\n0 &amp; 0 &amp; f\n\\end{bmatrix}\nSo we can say\nA =\n\\begin{bmatrix}\na &amp; b &amp; c \\\\\nxa &amp; xb + d &amp; xc + e \\\\\nya &amp; yb + zd &amp; yc + ze + f\n\\end{bmatrix}\n\\square\nMultiplying factorization method (like Gaussian elimination?)\nStarted with A = IA where I is identical matrix.\n3x3 example\nA =\n\\begin{bmatrix}\n1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\na &amp; b &amp; c \\\\\nd &amp; e &amp; f \\\\\ng &amp; h &amp; i\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 &amp; 0 &amp; 0 \\\\\n\\frac{d}{a} &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\na &amp; b &amp; c \\\\\n0 &amp; e - \\frac{bd}{a} &amp; f - \\frac{cd}{a} \\\\\ng &amp; h &amp; i\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 &amp; 0 &amp; 0 \\\\\n\\frac{d}{a} &amp; 1 &amp; 0 \\\\\n\\frac{g}{a} &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\na &amp; b &amp; c \\\\\n0 &amp; e - \\frac{bd}{a} &amp; f - \\frac{cd}{a} \\\\\n0 &amp; h - \\frac{bg}{a} &amp; i - \\frac{cg}{a}\n\\end{bmatrix}\nthen eliminate h-\\frac{bg}{a} term by first or second row etc. \\square\nPartial Pivot\nExchange rows to let the biggest element in first row.\nGiven by the formula PA = LU.\nBut there are types that no need to pivot at all:\n\nDiagonally dominant\nSPD, where we can use Cholesky factorization instead\n\nApplications\nAfter factorization, we can solve Ax=b by Forward-backward substitution.\nBasically Ax = b is equivalent to x = A^{-1}b.\nSherman-Morrison\nx = (A - uv^T)^{-1}b = A^{-1}b + A^{-1}u(1 - v^TA^{-1}u)^{-1}v^TA^{-1}b\nWe can solve those A^{-1}b and A^{-1}u by Forward-backward substitution.\nCosts\ngenerally total cost in O(n^3)\\approx \\frac{2n^3}{3} operations\nBut after enable partial pivoting (only row exchange), total cost is O(n^2) &lt;&lt; \\frac{2n^3}{3}\nHowever for full pivoting, it is still O(n^3) for additional searching cost\nCode\ncredit: UTexas.\nimport numpy as np\n \ndef LU(A):\n\tn = A.shape[0]\n\tU = A.copy()\n\tL = np.eye(n, dtype=np.double)\n\tfor i in range(n):\n\t\tfactor = U[i + 1:, i] / U[i, i]\n\t\tL[i + 1:, i] = factor\n\t\tU[i + 1:] -= factor[:, np.newaxis] * U[i]\n\treturn L, U"},"NumericalAnalysis/CH2/Orthogonal-matrix":{"slug":"NumericalAnalysis/CH2/Orthogonal-matrix","filePath":"NumericalAnalysis/CH2/Orthogonal matrix.md","title":"Orthogonal matrix","links":["NumericalAnalysis/CH2/Orthogonal-matrix"],"tags":[],"content":"A is m\\times n\nDefinition\n\nA^TA=I and AA^T = I.\n\\det(A) = \\pm1\nA^{-1} = A^T\ncolumns of Orthogonal matrix are mutually orthogonal\nA^T is Orthogonal matrix\n"},"NumericalAnalysis/CH2/Symmetric-positive-definite-matrix":{"slug":"NumericalAnalysis/CH2/Symmetric-positive-definite-matrix","filePath":"NumericalAnalysis/CH2/Symmetric positive definite matrix.md","title":"Symmetric positive definite matrix","links":["NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector"],"tags":[],"content":"A is m\\times n matrix\nDefinition\n\nA = A^T and (\\forall x \\neq 0) x^TAx &gt; 0.\nFor all 0 &lt; i &lt; n\n\nDiagonal entries a_{ii} &gt; 0\nEigenvalues \\lambda_i &gt; 0\nCondition number cond(A) = \\frac{\\max \\lambda}{\\min \\lambda}\n\n\n"},"NumericalAnalysis/CH2/index":{"slug":"NumericalAnalysis/CH2/index","filePath":"NumericalAnalysis/CH2/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH3/Hessenberg-matrix":{"slug":"NumericalAnalysis/CH3/Hessenberg-matrix","filePath":"NumericalAnalysis/CH3/Hessenberg matrix.md","title":"Hessenberg matrix","links":[],"tags":[],"content":"\nUpper:\nA is upper triangular with one additional nonzero diagonal below the main one: i &gt; j + 1 \\Longrightarrow A_{ij} = 0\nLower:\n"},"NumericalAnalysis/CH3/QR-factorization":{"slug":"NumericalAnalysis/CH3/QR-factorization","filePath":"NumericalAnalysis/CH3/QR factorization.md","title":"QR factorization","links":["NumericalAnalysis/CH2/Orthogonal-matrix","NumericalAnalysis/matrix","NumericalAnalysis/CH3/Hessenberg-matrix"],"tags":[],"content":"A = QR\nwhere:\n\nA is m\\times n\nR is square upper triangular matrix\nQ is Orthogonal matrix\nFeatures:\nQR factorization always exists but may not unique\nQR factorization only exists when Determinant is 0 and all diagonal elements &gt;0.\n\n\ngiven by numpy.linalg.qr\nSteps\nGram-Schmidt\n\nmost intuitive but not stable\nref: kwokanthony.medium.com/important-decomposition-in-linear-algebra-detailed-explanation-on-qr-decomposition-by-classical-3f8f5425915f\n\n\nfor column \\vec{a_i} in A, calculate u_i = a_i -(\\sum^{i-1}_{j=1} proj_{u_{j-1}} a_j) = a_i -(\\sum^{i-1}_{j=1} \\frac{a_j \\cdot u_{j-1}}{u_{j-1} \\cdot u_{j-1}} a_j).\nthen q_i = \\frac{u_i}{||u_i||} which is column in Q\nrepeat for all columns to get Q.\nR = Q^TA.\n\nHouseholder\n\nref: kwokanthony.medium.com/detailed-explanation-with-example-on-qr-decomposition-by-householder-transformation-5e964d7f7656\n\ncost O(n^3)\nSteps\n\ninit R_0 = A\nfor column \\vec{a_i} in A, calculate \\vec{v_i} = \\vec{a_i} \\pm ||\\vec{a_i}||_2e_i where e_1 is standard basis. Choose the sign to avoid cancellation.\nw_i = \\frac{\\vec{v_i}}{||\\vec{v_i}||}, Q_i = I - 2w_iw_i^T and R_i = Q_iR_{i - 1}.\nkeep this calculation column by column with only consider sub-matrix until R_i is upper right triangle matrix.\nQ = \\prod Q_i, R is last result of R_i.\n\n2x2 example\nA = \\begin{bmatrix}\n3 &amp; 7\\\\\n4 &amp; 2\n\\end{bmatrix}\n\\vec{v_1} = \\begin{bmatrix}3\\\\4\\end{bmatrix} \\pm \\sqrt{9 + 16} \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}8\\\\4\\end{bmatrix}\nw_1 = \\frac{1}{\\sqrt{64 + 26}}\\begin{bmatrix}8\\\\4\\end{bmatrix}\nQ_1 = I - 2w_1w_1^T = \\begin{bmatrix}\n-3/5 &amp; -4/5\\\\\n-4/5 &amp; 3/5\n\\end{bmatrix}\nThen\nR_1 = Q_1R_0 = Q_1A = \\begin{bmatrix}\n-5 &amp; -\\frac{29}{5}\\\\\n0 &amp; - \\frac{22}{5}\n\\end{bmatrix}\nSince R is already in right upper triangle matrix. We are done with R = R_1 and\nQ = Q_1 = \\begin{bmatrix}\n-\\frac{3}{5} &amp; -\\frac{4}{5}\\\\\n-\\frac{4}{5} &amp; \\frac{3}{5}\n\\end{bmatrix}\nIt’s consist with\n&gt;&gt;&gt; numpy.linalg.qr(np.matrix(&quot;3,7;4,2&quot;))  \nQRResult(Q=matrix([[-0.6, -0.8],  \n       [-0.8,  0.6]]), R=matrix([[-5. , -5.8],  \n       [ 0. , -4.4]]))\nGivens\n\nref: kwokanthony.medium.com/detailed-explanation-with-example-on-qr-decomposition-by-givens-rotation-6e7bf664fbdd\n\nG = \\begin{bmatrix}\n\\cos(\\theta) &amp; -\\sin(\\theta)\\\\\n\\sin(\\theta) &amp; \\cos(\\theta)\n\\end{bmatrix}\nFor higher dimensions, we need to use sth like\nG_{xy} = \\begin{bmatrix}\n\\cos(\\theta) &amp; -\\sin(\\theta) &amp; 0\\\\\n\\sin(\\theta) &amp; \\cos(\\theta) &amp; 0\\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\nG_{yz} = \\begin{bmatrix}\n1 &amp; 0 &amp; 0\\\\\n0 &amp; \\cos(\\theta) &amp; -\\sin(\\theta)\\\\\n0 &amp; \\sin(\\theta) &amp; \\cos(\\theta)\n\\end{bmatrix}\nG_{xz} = \\begin{bmatrix}\n\\cos(\\theta) &amp; 0 &amp; -\\sin(\\theta)\\\\\n0 &amp; 1 &amp; 0\\\\\n\\sin(\\theta) &amp; 0 &amp; \\cos(\\theta)\n\\end{bmatrix}\ndepend on which dimension.\nSteps\n\nFor column a_i in A, use G to build it to consist with right upper triangle matrix\ne.g. if we need to remove 2nd element in 3\\times 3 matrix, we need to use G_{xy} with cos(\\theta) = \\frac{x}{r} and \\sin(\\theta) = - \\frac{y}{r} where x is 1st element and y is the 2nd, and r = \\sqrt{x^2 + y^2}.\nThen get A_i = A_{i-1}G. Then check if we need to remove another element (like 3rd element) as well.\nRepeat this for all columns.\nQ = \\prod G_i^T, R is the last A_i.\n\nCosts\nif A is upper Hessenberg matrix, it only requires n Givens rotation.\notherwise O(n^2)."},"NumericalAnalysis/CH3/Singular-value-decomposition":{"slug":"NumericalAnalysis/CH3/Singular-value-decomposition","filePath":"NumericalAnalysis/CH3/Singular value decomposition.md","title":"Singular value decomposition","links":["NumericalAnalysis/CH2/Orthogonal-matrix","NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector"],"tags":[],"content":"A = U\\Sigma V^T\nwhere\n\nA is m\\times n\nU: An m\\times m Orthogonal matrix matrix whose columns are the left singular vectors of A.\n\\Sigma: A diagonal m\\times n matrix containing the singular values of A in descending order.\nV^T: The transpose of an n\\times n Orthogonal matrix where the columns are the right singular vectors of A.\n\nSteps\n\ncalculate AA^T\ncalculate Eigenvalues of AA^T by solving det(AA^T - \\lambda I) = 0, where I is identify matrix\nthen \\Sigma is \\delta from those \\lambda\ncalculate V right singular vectors(should be unit vectors), for each \\lambda, (A^TA - \\lambda I)v = 0\ncalculate U left singular vectors(should be unit vectors), for each \\lambda, (AA^T - \\lambda I)u = 0\n\n3x2 example\n\nref www.geeksforgeeks.org/singular-value-decomposition-svd/\n\nA=\\begin{bmatrix}\n1 &amp; 2 &amp; 3 \\\\\n3 &amp; 2 &amp; 1\n\\end{bmatrix}\nAA^T = \\begin{bmatrix}\n1 &amp; 2 &amp; 3 \\\\\n3 &amp; 2 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; 3 \\\\\n2 &amp; 2 \\\\\n3 &amp; 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 + 4 + 9 &amp; 3 + 4 + 3 \\\\\n3 + 4 + 3 &amp; 9 + 4 + 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n14 &amp; 10\\\\\n10 &amp; 14\n\\end{bmatrix}\n\\det(AA^T - \\lambda I) =\n\\begin{bmatrix}\n14 - \\lambda &amp; 10 \\\\\n10 &amp; 14 - \\lambda\n\\end{bmatrix} = (14 - \\lambda)^2 - 10^2 = 14^2 - 28\\lambda + \\lambda^2 - 10^2 = \\lambda^2 - 28\\lambda + 96\n(\\lambda - 24)(\\lambda - 4) = 0\nThen we can get some \\lambda Eigenvalues, \\lambda_1 = 24 and \\lambda_2 = 4 and Singular value \\delta_1 = \\sqrt{24} and \\delta_2 = 2.\nwe can say\n\\Sigma = \\begin{bmatrix}\n\\delta_1 &amp; 0 &amp; 0 \\\\\n0 &amp; \\delta_2 &amp; 0\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\sqrt{24} &amp; 0 &amp; 0 \\\\\n0 &amp; 2 &amp; 0\n\\end{bmatrix}\nFor each of them, calculate eigenvectors (A^TA - \\lambda I)v = 0\nwhere\nA^TA=\\begin{bmatrix}\n10 &amp; 8 &amp; 6 \\\\\n8 &amp; 8 &amp; 8 \\\\\n6 &amp; 8 &amp; 10\n\\end{bmatrix}\n(A^TA-24I)v_1 = \\begin{bmatrix}\n-14 &amp; 8 &amp; 6 \\\\\n8 &amp; -16 &amp; 8 \\\\\n6 &amp; 8 &amp; -14\n\\end{bmatrix}v_1 = 0\nRREF it\n\\begin{bmatrix}\n1 &amp; 0 &amp; -1 \\\\\n0 &amp; 1 &amp; -1 \\\\\n0 &amp; 0 &amp; 0\n\\end{bmatrix}v_1 = 0\nso x - z = 0 and y - z = 0\nv_1 = \\begin{bmatrix}\n\\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{3}} \n\\end{bmatrix}\nso do \\lambda_2 = 4\n(A^TA-4I)v_2 = \\begin{bmatrix}\n6 &amp; 8 &amp; 6 \\\\\n8 &amp; 4 &amp; 8 \\\\\n6 &amp; 8 &amp; 6\n\\end{bmatrix}v_2 = 0\nRREF it\n\\begin{bmatrix}\n1 &amp; 0 &amp; 1 \\\\\n0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 0\n\\end{bmatrix}\nso x + z = 0 and y = 0\nv_2 = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} \\\\\n0 \\\\\n\\frac{-1}{\\sqrt{2}}\n\\end{bmatrix}\nThen for v_3, since it must perpendicular to v_1,v_2, we can say v_1^Tv_3 = 0 and v_2^Tv_3 = 0 is true. Solve these two equations to get v_3.\nv_3 = \\begin{bmatrix}\n\\frac{1}{\\sqrt{6}} \\\\\n-\\frac{2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{6}}\n\\end{bmatrix}\nThen\nV =\\begin{bmatrix}\n*v_1 &amp; *v_2 &amp; *v_3\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} &amp; \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{6}}\\\\\n\\frac{1}{\\sqrt{3}} &amp; 0 &amp; \\frac{-2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{3}} &amp; \\frac{-1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{6}}\\\\\n\\end{bmatrix}\nLastly need to find U with similar steps.\n(AA^T - 24I)u_1 = \\begin{bmatrix}\n-10 &amp; 10 \\\\\n10 &amp; -10\n\\end{bmatrix}u_1 = 0\nRREF it\n\\begin{bmatrix}\n-1 &amp; 1 \\\\\n0 &amp; 0 \\\\\n\\end{bmatrix}\nso u_1 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}\\end{bmatrix}\n(AA^T - 4I)u_2 = \\begin{bmatrix}\n10 &amp; 10 \\\\\n10 &amp; 10\n\\end{bmatrix}u_1 = 0\nRREF it\n\\begin{bmatrix}\n1 &amp; 1 \\\\\n0 &amp; 0 \\\\\n\\end{bmatrix}\nso u_2 = \\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}}\\end{bmatrix}\nThen\nU = \\begin{bmatrix}*u_1 &amp; *u_2\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}}\n\\end{bmatrix}\nTherefore\nA = U\\Sigma V^T = \\begin{bmatrix}\n\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sqrt{24} &amp; 0 &amp; 0 \\\\\n0 &amp; 2 &amp; 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} &amp; \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{6}}\\\\\n\\frac{1}{\\sqrt{3}} &amp; 0 &amp; \\frac{-2}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{3}} &amp; \\frac{-1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{6}}\\\\\n\\end{bmatrix}^T\n\\square\nApplications\nPseudo Inverse (Moore-Penrose Inverse)\nM = U\\Sigma V^T"},"NumericalAnalysis/CH3/Vandermonde-matrix":{"slug":"NumericalAnalysis/CH3/Vandermonde-matrix","filePath":"NumericalAnalysis/CH3/Vandermonde matrix.md","title":"Vandermonde matrix","links":[],"tags":[],"content":"Definition\nMatrix A n\\times n has basis function t^i. Every term is t_i^j where j \\in [0, n - 1] and i \\in [1,n].\nExample\n&gt;&gt;&gt; import numpy\n&gt;&gt; numpy.vander([1, 2, 3, 4, 5], N=5, increasing=True)  \narray([[  1,   1,   1,   1,   1],  \n      [  1,   2,   4,   8,  16],  \n      [  1,   3,   9,  27,  81],  \n      [  1,   4,  16,  64, 256],  \n      [  1,   5,  25, 125, 625]])"},"NumericalAnalysis/CH3/index":{"slug":"NumericalAnalysis/CH3/index","filePath":"NumericalAnalysis/CH3/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH3/normal-equation":{"slug":"NumericalAnalysis/CH3/normal-equation","filePath":"NumericalAnalysis/CH3/normal equation.md","title":"normal equation","links":["NumericalAnalysis/CH3/pseudo-inverse-(Moore-Penrose-Inverse)","NumericalAnalysis/CH3/Vandermonde-matrix","NumericalAnalysis/CH3/normal-equation"],"tags":[],"content":"Definition\nSolution is\nx = (A^TA)^{-1}A^Tb\nIf (A^TA)^{-1} such inverse not exists, we need to use x \\approx A^+b from pseudo inverse (Moore-Penrose Inverse) to solve it.\nExample of solving polynomial\nf(x) = c_0 + c_1x + c_2x^2 + c_3x^3\nGiven vector x and f as input and output, we want to find c coefficients.\n\nbuild Vandermonde matrix\nSolve it by normal equation / pseudo inverse (Moore-Penrose Inverse)\n\n# build Vandermonde matrix\nA = np.vander(x, 4, True)\nc = la.solve(A.T @ f, A.T @ A)"},"NumericalAnalysis/CH3/pseudo-inverse-(Moore-Penrose-Inverse)":{"slug":"NumericalAnalysis/CH3/pseudo-inverse-(Moore-Penrose-Inverse)","filePath":"NumericalAnalysis/CH3/pseudo inverse (Moore-Penrose Inverse).md","title":"pseudo inverse (Moore-Penrose Inverse)","links":["NumericalAnalysis/matrix","NumericalAnalysis/CH3/Singular-value-decomposition","NumericalAnalysis/CH3/normal-equation"],"tags":[],"content":"A no-square m \\times n matrix A usually has no inverse.\nBy Rank of matrix\n\nif rank(A) = n we can say A^+ = (A^TA)^{-1}A^T and cond(A) = ||A^T||_2 \\cdot ||A^+||_2.\nif rank(A) &lt; n, we can say cond(A) = \\infty.\nA simple construction method by Singular value decomposition is\n\nA^+ = V\\Sigma^+U^T\nwhere \\Sigma^+ is (\\forall \\delta \\in \\Sigma &gt; 0) \\delta^+ = \\frac{1}{\\delta}.\nApplications\nReplacing normal equation to solve polynomial when (A^TA) doesn’t have inverse: least squares solution of Ax \\approx b is given by x = A^+b."},"NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector":{"slug":"NumericalAnalysis/CH4/Eigenvalue-and-Eigenvector","filePath":"NumericalAnalysis/CH4/Eigenvalue and Eigenvector.md","title":"Eigenvalue and Eigenvector","links":[],"tags":[],"content":"Eigenvalues\n\nNotation: \\lambda\n\nsolve \\det(A - \\lambda I) = 0\ngiven by numpy.linalg.eighfor diagonal matrix and numpy.linalg.eig for non.\nSingular value\n\nNotation: \\delta\n\n\\delta^2 = \\lambda\nEigenvector\nAx = \\lambda x\nwhere x is Eigenvector and \\lambda is Eigenvalues.\nApplications\nProblem transformations\n\nShift, (A - \\sigma I)x = (\\lambda - \\sigma)x where \\sigma is scalar.\nInversion, A^{-1}x = \\frac{x}{\\lambda} if A nonsingular and x \\neq 0.\nPowers, A^kx = \\lambda^kx.\nPolynomial, p(A)x = p(\\lambda)x where p(t) is polynomial.\n\nExample\nFor A is symmetric matrix, e^A can be done by\ne^A = Xe^{\\lambda}X^T\nwhere X is Eigenvector matrix\nev, v = np.linalg.eigh(A)\nexpmA = v @ np.diag(np.exp(ev)) @ v^T"},"NumericalAnalysis/CH4/Krylov-subspace":{"slug":"NumericalAnalysis/CH4/Krylov-subspace","filePath":"NumericalAnalysis/CH4/Krylov subspace.md","title":"Krylov subspace","links":["NumericalAnalysis/CH2/Symmetric-positive-definite-matrix"],"tags":[],"content":"\nshifting does not improve Lanczos / Arnoldi\n\nConcept\nx_k = Ax_{k-1}\n\\lambda = \\frac{x^T_kAx_k}{x^T_kx_k}\nLanczos iteration for symmetric\nimport numpy as np\n \ndef lanczos_iter(A, tol=1e-8):\n    n = A.shape[0]\n    \n    # q0 = np.zeros(n)  # q0 = 0\n    # b0 = 0            # b0 = 0\n    x0 = np.random.rand(n)\n    q1 = x0 / np.linalg.norm(x0)\n\t\n    Q = [q1]  # start with k = 1\n    alphas = []\n    betas = [b0]\n\t\n    qk = q1\n    qk_prev = q0\n    bk_prev = b0\n    \n    for k in range(1, n+1):\n        # u_k = A*q_k\n        uk = A @ qk\n\t\t\n\t\t# .T same as .H cause it&#039;s real\n        # a_k = q_k^T * u_k\n        ak = qk.T @ uk\n        alphas.append(ak)\n        \n        # u_k = u_k - b_{k-1}*q_{k-1} - a_k*q_k\n        uk = uk - bk_prev * qk_prev - ak * qk\n        \n        # b_k = ||u_k||\n        bk = np.linalg.norm(uk)\n\t    \n        if abs(bk) &lt; tol:\n            break\n            \n        betas.append(bk)\n        \n        # q_{k+1} = u_k/b_k\n        qk_next = uk / bk\n        Q.append(qk_next)\n        \n\t    \n        qk_prev = qk\n        qk = qk_next\n        bk_prev = bk\n    \n    \n    m = len(alphas)\n    T = np.zeros((m, m))\n    np.fill_diagonal(T, alphas)\n    for i in range(m-1):\n        T[i, i+1] = betas[i+1]\n        T[i+1, i] = betas[i+1]\n\t    \n    return np.column_stack(Q), T\nOptimization\nturn A to Symmetric positive definite matrix.\nArnoldi iteration for nonsymmetric\nimport numpy as np\n \ndef arnoldi_iter(A, tol=1e-8):\n    n = A.shape[0]\n    x0 = np.random.rand(n)\n    q1 = x0 / np.linalg.norm(x0)\n    \n    # Initialize properly\n    Q = np.zeros((n, n+1))\n    Q[:, 0] = q1\n    H = np.zeros((n+1, n))\n    \n    for k in range(n):\n        v = A @ Q[:, k]\n        \n        # Orthogonalization\n        for j in range(k+1):\n            H[j, k] = Q[:, j].T @ v\n            v = v - H[j, k] * Q[:, j]\n        \n        H[k+1, k] = np.linalg.norm(v)\n        \n        if H[k+1, k] &lt; tol:\n            break\n            \n        Q[:, k+1] = v / H[k+1, k]\n    \n    return Q[:, :k+1], H[:k+2, :k+1]"},"NumericalAnalysis/CH4/QR-iteration":{"slug":"NumericalAnalysis/CH4/QR-iteration","filePath":"NumericalAnalysis/CH4/QR iteration.md","title":"QR iteration","links":["NumericalAnalysis/CH3/QR-factorization"],"tags":[],"content":"Steps\nstarts with A_1 = A and repeat for k=1,2,...\nQ_kR_k = A_k by QR factorization\nA_{k+1}=R_kQ_k\nwhen A_{k+1}-A_k &lt; tol, stop\nafter convergence,\nThe diagonal entries of A are the corresponding eigenvalues.\nEigenvectors are the columns of matrix Q = \\prod_j^k Q_j.\nCost\n\nSymmetric\n\n\\frac{4}{3}n^3 for only eigenvalues\n9n^3 for both eigenvalues and eigenvectors\n\n\nGeneral\n\n10n^3 for eigenvalues only\n25n^3 for both\n\n\n"},"NumericalAnalysis/CH4/Rayleigh-quotient-iteration":{"slug":"NumericalAnalysis/CH4/Rayleigh-quotient-iteration","filePath":"NumericalAnalysis/CH4/Rayleigh quotient iteration.md","title":"Rayleigh quotient iteration","links":["NumericalAnalysis/CH4/Rayleigh-quotient","NumericalAnalysis/CH4/inverse-iteration"],"tags":[],"content":"Concept\nBasically Rayleigh quotient with inverse iteration\n\\sigma_k = \\frac{x^T_kAx_k}{x^T_kx_k}\n(A - \\sigma_kI)y_{k+1} = x_k\nx_{k+1} = \\frac{y_{k+1}}{||y_{k+1}||_\\infty}\nFeatures\n\nhigh cost in every iteration since we are apply different shift.\n\nCode\nimport numpy as np\n \ndef rayleigh_quotient_iteration(A, num_iter, tol=1e-6):\n\tx_k = np.random.rand(A.shape[1])\n\tfor _ in range(num_iter):\n\t\tsigma = x_k.T @ A @ x_k / (x_k.T @ x_k)\n\t\ty_k = np.linalg.solve(A - sigma * np.eye(A.shape[0]), x_k)\n\t\tx_k_o = x_k\n\t\tx_k = y_k / np.linalg.norm(y_k)\n\t\tif np.linalg.norm(x_k - x_k_o) &lt; tol:\n\t\t\tbreak\n\treturn x_k"},"NumericalAnalysis/CH4/Rayleigh-quotient":{"slug":"NumericalAnalysis/CH4/Rayleigh-quotient","filePath":"NumericalAnalysis/CH4/Rayleigh quotient.md","title":"Rayleigh quotient","links":[],"tags":[],"content":"approximation eigenvalue as initial guess\nx\\lambda \\cong Ax\nx^Tx\\lambda = x^TAx\n\\lambda = \\frac{x^T_kAx_k}{x^T_kx_k}"},"NumericalAnalysis/CH4/Similarity-Transformation":{"slug":"NumericalAnalysis/CH4/Similarity-Transformation","filePath":"NumericalAnalysis/CH4/Similarity Transformation.md","title":"Similarity Transformation","links":[],"tags":[],"content":"if there exists\nB = P^{-1}AP\nwhere A, B and P are n by n matrices"},"NumericalAnalysis/CH4/deflation":{"slug":"NumericalAnalysis/CH4/deflation","filePath":"NumericalAnalysis/CH4/deflation.md","title":"deflation","links":["NumericalAnalysis/CH4/Similarity-Transformation"],"tags":[],"content":"use Similarity Transformation like\nB = HAH^{-1}\nHouseholder transformation\nwe can use Householder transformation as H\ndef deflate_matrix(A, eigenvector, eigenvalue):\n    # Normalize the eigenvector\n    v = eigenvector / np.linalg.norm(eigenvector)\n    \n    # Householder matrix: H = I - 2vv^T\n    H = np.eye(A.shape[0]) - 2 * np.outer(v, v)\n    \n    # Deflated matrix: A&#039; = H A H\n    # bc H^{-1} = H in this case\n    A_deflated = H @ A @ H\n    \n    return A_deflated"},"NumericalAnalysis/CH4/index":{"slug":"NumericalAnalysis/CH4/index","filePath":"NumericalAnalysis/CH4/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH4/inverse-iteration":{"slug":"NumericalAnalysis/CH4/inverse-iteration","filePath":"NumericalAnalysis/CH4/inverse iteration.md","title":"inverse iteration","links":["NumericalAnalysis/CH4/power-iteration","NumericalAnalysis/CH2/LU-factorization","NumericalAnalysis/CH2/Forward-backward-substitution"],"tags":[],"content":"Concept\nAy_k = x_{k - 1}\nx_k = \\frac{y_k}{||y_k||_\\infty}\nwhich is equivalent to power iteration with A^{-1}.\nCode\nimport numpy as np\n \ndef inverse_iteration(A, num_iter, tol=1e-6):\n\tx_k = np.random.rand(A.shape[1])\n\tfor _ in range(num_iter):\n\t\ty_k = np.linalg.solve(A, x_k)\n\t\tx_k_o = x_k\n\t\tx_k = y_k / np.linalg.norm(y_k)\n\t\tif np.linalg.norm(x_k - x_k_o) &lt; tol:\n\t\t\tbreak\n\treturn x_k\nOr we can also reuse LU factorization with Forward-backward substitution instead of linalg.solve.\nVariants\nwith Shift\nuse A&#039; = A - \\sigma I for scalar \\sigma."},"NumericalAnalysis/CH4/orthogonal-iteration":{"slug":"NumericalAnalysis/CH4/orthogonal-iteration","filePath":"NumericalAnalysis/CH4/orthogonal iteration.md","title":"orthogonal iteration","links":["NumericalAnalysis/CH4/QR-iteration","NumericalAnalysis/CH3/QR-factorization"],"tags":[],"content":"\nalso called subspace iteration or simultaneous iteration\nit let us compute n dimensional rather one by each time\n\nequivalent to QR iteration\nSteps\nstart U_0 = I and repeat following for k=1,2,...\nV_k = AU_{k-1}\nU_kR_k = V_k by QR factorization\nwhen U_k - U_{k-1} &lt; tol, we say it converge and stop\nAfter convergence:\n\nCompute the Rayleigh quotient matrix: H = U^T A U  where U is last U_k\nFind eigen-decomposition of H: HQ = QD, where D is diagonal\nCompute eigenvectors of original matrix A: X = UQ\nThe diagonal entries of D are the corresponding eigenvalues\n"},"NumericalAnalysis/CH4/power-iteration":{"slug":"NumericalAnalysis/CH4/power-iteration","filePath":"NumericalAnalysis/CH4/power iteration.md","title":"power iteration","links":[],"tags":[],"content":"Concept\nb_{k+1} = Ab_k\nConvergent rate\n\\frac{\\lambda_2}{\\lambda_1}\nwhere \\lambda_2 is eigenvalue having 2nd largest modulus\nLimitations\n\nmay not converge\nbad initial guess vector\nmore than on eigenvector, so it will converge to linear combination of those two\n\nVariants\nnormalized power iteration\nConcept\ny_k = Ax_{k-1}\nx_k = \\frac{y_k}{||y_k||_\\infty}\nwhere every iteration it been multiply by a matrix A and normalized.\nIt’s goal is to find a eigenvector of A.\nCode\nimport numpy as np\n \ndef power_iter(A, num_iter, tol=1e-6):\n\tx_k = np.random.rand(A.shape[1])\n\tfor _ in range(num_iter):\n\t\ty_k = np.dot(A, x_k)\n\t\tx_k_o = x_k\n\t\tx_k = y_k / np.linalg.norm(y_k)\n\t\tif np.linalg.norm(x_k - x_k_o) &lt; tol:\n\t\t\tbreak\n\treturn x_k\npower iteration with Shift\nin order to accelerate the convergence speed, we can pick a shift \\sigma where A&#039; = A - \\sigma I and\n\\frac{\\lambda_2 - \\sigma}{\\lambda_1 - \\sigma} &lt; \\frac{\\lambda_2}{\\lambda_1}\neverything else keep same. After all, you need to add shift into result."},"NumericalAnalysis/CH5/Aitken's-method":{"slug":"NumericalAnalysis/CH5/Aitken's-method","filePath":"NumericalAnalysis/CH5/Aitken's method.md","title":"Aitken's method","links":[],"tags":[],"content":"Concept\ny_{k+2} = x_{k+2} - \\frac{\\Delta_2^2}{\\Delta_2 - \\Delta_1}\nwhere\n\\Delta_1 = x_{k + 1} - x_k\n\\Delta_2 = x_{k + 2} - x_{k + 1}\nthis method is used to accelerating linearly converge\nfor k = 0, 2, 4, ...\n\tx_{k + 1} = g(x_k)\n\tx_{k + 2} = g(x_{k + 1})\n\tcalc deltas\n\tx_{k + 2} = x_{k + 2} - Delta_2^2 / (Detla_2 - Delta_1)\n"},"NumericalAnalysis/CH5/Broyden’s-Method":{"slug":"NumericalAnalysis/CH5/Broyden’s-Method","filePath":"NumericalAnalysis/CH5/Broyden’s Method.md","title":"Broyden’s Method","links":[],"tags":[],"content":""},"NumericalAnalysis/CH5/Jacobian-matrix":{"slug":"NumericalAnalysis/CH5/Jacobian-matrix","filePath":"NumericalAnalysis/CH5/Jacobian matrix.md","title":"Jacobian matrix","links":[],"tags":[],"content":"Concept\n[J(x_k)]_{ij} = \\left[\\frac{\\partial f(x_i)}{\\partial x_j}\\right]\nExample\n\\begin{pmatrix}\nf_1\\\\\nf_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ne^{x_1x_2}\\\\\ne^{x_1} + e^{x_2}\n\\end{pmatrix}\nJ = \\begin{pmatrix}\n\\frac{\\partial f_1}{x_1} &amp; \\frac{\\partial f_1}{x_2}\\\\\n\\frac{\\partial f_2}{x_1} &amp; \\frac{\\partial f_2}{x_2}\n\\end{pmatrix}\n= \\begin{pmatrix}\nx_2e^{x_1x_2} &amp; x_1e^{x_1x_2}\\\\\ne^{x_1} &amp; e^{x_2}\n\\end{pmatrix}"},"NumericalAnalysis/CH5/Muller's-method":{"slug":"NumericalAnalysis/CH5/Muller's-method","filePath":"NumericalAnalysis/CH5/Muller's method.md","title":"Muller's method","links":[],"tags":[],"content":""},"NumericalAnalysis/CH5/Newton's-method---multi":{"slug":"NumericalAnalysis/CH5/Newton's-method---multi","filePath":"NumericalAnalysis/CH5/Newton's method - multi.md","title":"Newton's method - multi","links":["NumericalAnalysis/CH5/Jacobian-matrix","NumericalAnalysis/CH5/Broyden’s-Method"],"tags":[],"content":"Concept\nx_{k + 1} = x_k - J^{-1}(x_k)f(x_k)\nwhere J is Jacobian matrix\nCost\nn^2 to evaluate Jacobian matrix\nCosts\nsolve it cost O(n^3)\nFor evaluating Jacobian matrix, it’s n^2\nVariants\nsolving it cost O(n^3).\n\nBroyden’s Method\n"},"NumericalAnalysis/CH5/Newton's-method":{"slug":"NumericalAnalysis/CH5/Newton's-method","filePath":"NumericalAnalysis/CH5/Newton's method.md","title":"Newton's method","links":["NumericalAnalysis/CH5/convergence-rate","NumericalAnalysis/CH5/multiplicity","NumericalAnalysis/CH5/Secant's-method","NumericalAnalysis/CH5/Muller's-method"],"tags":[],"content":"Concept\nx_{k + 1} = x_k - \\frac{f(x_k)}{f&#039;(x_k)}\nsometime we use g(x)\ng(x) = x - \\frac{f(x)}{f&#039;(x)}\nand\ng&#039;(x) = \\frac{f(x)f&#039;&#039;(x)}{f&#039;(x)^2}\nExamples\n\\frac{1}{A}\nf(x) = A - \\frac{1}{x}\nthen f(\\frac{1}{A}) = 0.\ng(x) = x - \\frac{f}{f} = x - (Ax^2 - x) = 2x - Ax^2\n\\sqrt{A}\nf(x) = x^2 - A\nand\ng(x) = x - \\frac{x^2 - A}{2x} = \\frac{1}{2}(x + \\frac{A}{x})\nconvergence rate\nconvergence rate for simple root of Newton’s method is quadratic (r = 2).\nfor multiple root(multiplicity m &gt; 1), it has linear rate.\nVariants\n\nSecant’s method\nMuller’s method\n"},"NumericalAnalysis/CH5/Secant's-method":{"slug":"NumericalAnalysis/CH5/Secant's-method","filePath":"NumericalAnalysis/CH5/Secant's method.md","title":"Secant's method","links":["NumericalAnalysis/CH5/convergence-rate"],"tags":[],"content":"Concept\nx_{k + 1} = x_k - f(x_k)\\frac{x_k - x_{k - 1}}{f(x_k) - f(x_{k - 1})}\nit has super-linear convergence rate, with r \\approx 1.618."},"NumericalAnalysis/CH5/bisection-method":{"slug":"NumericalAnalysis/CH5/bisection-method","filePath":"NumericalAnalysis/CH5/bisection method.md","title":"bisection method","links":["NumericalAnalysis/CH5/convergence-rate"],"tags":[],"content":"Concept\nm = a + (b - a )/2\nand if sign(f(a)) == sign(f(m)), a = m else b = m\nrate\nit should has linear convergence rate since the interval cut half every iteration and C = 0.5."},"NumericalAnalysis/CH5/convergence-rate":{"slug":"NumericalAnalysis/CH5/convergence-rate","filePath":"NumericalAnalysis/CH5/convergence rate.md","title":"convergence rate","links":["RealAnalysis/sequences/convergent"],"tags":[],"content":"sequence converges rate\nif\n\\lim_{k\\to \\infty} \\frac{||e_{k+1}||}{||e_k||^r} = C\nwhere C is finite nonzero constant, we can say it convergent with rate r\nand\n\nr = 1: linear (C &lt; 1)\nr &gt; 1: super linear\nr = 2: quadratic\n\nalso\n\n|g&#039;(x^*)|&lt;1 it can converge given acceptable interval\n|g&#039;(x^*)| &gt; 1 it will diverge\n|g&#039;(x^*)| = 0 it’s at least quadratic\n"},"NumericalAnalysis/CH5/index":{"slug":"NumericalAnalysis/CH5/index","filePath":"NumericalAnalysis/CH5/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH5/inverse-quadratic-interpolation":{"slug":"NumericalAnalysis/CH5/inverse-quadratic-interpolation","filePath":"NumericalAnalysis/CH5/inverse quadratic interpolation.md","title":"inverse quadratic interpolation","links":[],"tags":[],"content":"Concept\nfor every iteration\nu=f(b)/f(c), v = f(b)/f(a), w = f(a)/f(c)\np = v(w(u-w)(c-b - (1-u)(b-a)))\nq = (w-1)(u-1)(v-1)\nthen\nc = a, a = b and b = b + p"},"NumericalAnalysis/CH5/multiplicity":{"slug":"NumericalAnalysis/CH5/multiplicity","filePath":"NumericalAnalysis/CH5/multiplicity.md","title":"multiplicity","links":["NumericalAnalysis/CH5/multiplicity"],"tags":[],"content":"Concept\nIf f(x^*) = f&#039;(x^*) = f&#039;&#039;(x^*) = \\cdots = f^{(m - 1)}(x^*) = 0 and f^{(m)}(x^*) \\neq 0, we can say x^* is a solution with multiplicity of m.\nExample\ny = (x - 1)^3 indicated 1 is a root/solution with multiplicity of 3"},"NumericalAnalysis/CH6/Hessian-matrix":{"slug":"NumericalAnalysis/CH6/Hessian-matrix","filePath":"NumericalAnalysis/CH6/Hessian matrix.md","title":"Hessian matrix","links":["NumericalAnalysis/matrix","NumericalAnalysis/CH6/Hessian-matrix"],"tags":[],"content":"Concept\n[H_f(x)]_{ij} = \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}\nwhich is symmetric.\nproperty\nAt critical point, H_f(x^*) is\n\npositive definite \\Longrightarrow x^* is minimum\nnegative definite \\Longrightarrow x^* is maximum\nindefinite  \\Longrightarrow x^* is saddle point\nsingular, then various pathological situations are possible\n\nExample\nf = 1 + (x-1)^2 + (x-4)^2\nHessian matrix is 1 \\times 1 symmetric matrix."},"NumericalAnalysis/CH6/Lagrangian-function":{"slug":"NumericalAnalysis/CH6/Lagrangian-function","filePath":"NumericalAnalysis/CH6/Lagrangian function.md","title":"Lagrangian function","links":["NumericalAnalysis/CH5/Jacobian-matrix","NumericalAnalysis/CH6/Hessian-matrix"],"tags":[],"content":"Concept\n\\mathcal{L}(x, \\lambda) = f(x) + \\lambda^Tg(x)\nwhere \\lambda is vector of Lagrangian multipliers, g is constrains function, and\n\\nabla \\mathcal{L}(x, \\lambda) = \\begin{bmatrix}\n\\nabla f(x) + J^T_g\\lambda\\\\\ng(x)\n\\end{bmatrix}\nwhere J is Jacobian matrix.\n\\nabla H_{\\mathcal{L}}(x, \\lambda) = \\begin{bmatrix}\nB(x, \\lambda) &amp; J^T_g(x)\\\\\nJ_g(x) &amp; O\n\\end{bmatrix}\nwhere O is a matrix with zeros and\nB(x, \\lambda) = H_f(x) + \\sum^m_{i = 1}\\lambda_iH_{gi}(x)\nwhere H is Hessian matrix."},"NumericalAnalysis/CH6/coercive":{"slug":"NumericalAnalysis/CH6/coercive","filePath":"NumericalAnalysis/CH6/coercive.md","title":"coercive","links":["RealAnalysis/sequences/continue","RealAnalysis/sequences/bounded","NumericalAnalysis/CH6/coercive"],"tags":[],"content":"Concept for function\ncontinue function f on unbounded(def of bounded) set S \\subseteq \\mathbb{R}^n is coercive if\n\\lim_{||x||\\to\\infty} f(x) = +\\infty\nExample\nx^4 - 3x^3 + 2x + 3 is coercive.\nbut x^3 + 2x^2 - x + 5 is not coercive."},"NumericalAnalysis/CH6/convex":{"slug":"NumericalAnalysis/CH6/convex","filePath":"NumericalAnalysis/CH6/convex.md","title":"convex","links":["RealAnalysis/sequences/convex-set"],"tags":[],"content":"convex set\n\ncheck the definition in real analysis here convex set\n\nSet S \\subseteq \\mathbb{R}^n is a convex set if it contains a line segment between any two of it’s points.\n\nconvex function\nFunction f: S \\subseteq \\mathbb{R}^n \\longrightarrow \\mathbb{R} is convex function on convex set S if it’s graph along any line segment in S lies on or below chord connecting function values at endpoints of segment.\n\nDef\nAny local minimum of convex function f on convex set S is global minimum of f on S.\nDef\nAny local minimum of strictly convex function f on convex set S is unique global minimum of f on S."},"NumericalAnalysis/CH6/critical-points":{"slug":"NumericalAnalysis/CH6/critical-points","filePath":"NumericalAnalysis/CH6/critical points.md","title":"critical points","links":["NumericalAnalysis/CH6/gradient"],"tags":[],"content":"Concept\nwhen gradient\n\\nabla f = 0"},"NumericalAnalysis/CH6/gradient":{"slug":"NumericalAnalysis/CH6/gradient","filePath":"NumericalAnalysis/CH6/gradient.md","title":"gradient","links":[],"tags":[],"content":"\\nabla f"},"NumericalAnalysis/CH6/index":{"slug":"NumericalAnalysis/CH6/index","filePath":"NumericalAnalysis/CH6/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH6/level-set":{"slug":"NumericalAnalysis/CH6/level-set","filePath":"NumericalAnalysis/CH6/level set.md","title":"level set","links":["RealAnalysis/sequences/continue","RealAnalysis/sequences/closed","RealAnalysis/sequences/bounded","NumericalAnalysis/CH6/coercive"],"tags":[],"content":"def\nlevel set is set of all points for which f has some given constant value\nsublevel set\nsublevel set is L_\\gamma = \\{x \\in S : f(x) \\leq \\gamma\\}\nDef\nIf a continue function f on S\\subseteq \\mathbb{R}^n has nonempty sublevel set and the set is closed and bounded, then f has global minimum on S.\nDef\nif S is unbounded, then f is coercive on S if and only if all of it’s sublevel set ate bounded."},"NumericalAnalysis/CH6/unimodal":{"slug":"NumericalAnalysis/CH6/unimodal","filePath":"NumericalAnalysis/CH6/unimodal.md","title":"unimodal","links":["NumericalAnalysis/CH6/unimodal"],"tags":[],"content":"Concept\nreal value function f is unimodal on [a, b] if there is unique x^* such that f(x^*) is minimum of f on [a,b] and f is strictly decreasing for x \\leq x^* and strictly increasing for x \\geq x^*."},"NumericalAnalysis/CH7/Chebyshev-Polynomials":{"slug":"NumericalAnalysis/CH7/Chebyshev-Polynomials","filePath":"NumericalAnalysis/CH7/Chebyshev Polynomials.md","title":"Chebyshev Polynomials","links":["NumericalAnalysis/CH7/Chebyshev-Polynomials"],"tags":[],"content":"Concept\nstarted with T_0 = 1 and T_1 = x\nT_{k + 1}(x) = 2xT_k(x) - T_{k - 1}(x)\n\nor\nT_j(x) = cos(j⋅arccos(x))\ntherefore it always only defined in [-1,1]\nWeight\nfor first-kind Chebyshev Polynomials\nw(t)=\\sqrt{1-t^2}"},"NumericalAnalysis/CH7/Chebyshev-interpolation":{"slug":"NumericalAnalysis/CH7/Chebyshev-interpolation","filePath":"NumericalAnalysis/CH7/Chebyshev interpolation.md","title":"Chebyshev interpolation","links":["NumericalAnalysis/CH7/Lagrange-Interpolation","NumericalAnalysis/CH7/orthogonal-polynomial-interpolation"],"tags":[],"content":"Definition\ninterpolation on top of Chebyshev node\nNode\nThe selection of node can be:\n\nChebyshev–Gauss: x_j=\\cos(\\pi\\frac{2j + 1}{2n})\nChebyshev–Lobatto\netc\n\nImplementation\nit can be implemented by Lagrange Interpolation, orthogonal polynomial interpolation, and etc.\nOR\nimport numpy as np\nimport numpy.linalg as la\n \nn = len(y)\nx = np.cos((2 * np.array(list(range(n))) + 1)*np.pi/(2*n))\nV = np.zeros((n,n))\nV[:,0] = 1\nV[:,1] = x\nfor i in range(2, n):\n    V[:,i] = 2*x*V[:,i-1] - V[:,i-2]\n \nc = la.solve(V,y)\nCrude Error\n\\max |f(x) - p_{n-1}(x)| \\approx \\frac{C}{2^{n-1}}(b-a)\n^n$$\nwhere\n- $C$ constant\n- $n$ number of nodes\n- $b-a$ the range"},"NumericalAnalysis/CH7/Lagrange-Interpolation":{"slug":"NumericalAnalysis/CH7/Lagrange-Interpolation","filePath":"NumericalAnalysis/CH7/Lagrange Interpolation.md","title":"Lagrange Interpolation","links":[],"tags":[],"content":"Concept\nThe basis function defined as\nl_j(t_i) = \\delta_{ij} = \\begin{cases}\n1 &amp;, i = j\\\\\n0 &amp;, i \\neq j\n\\end{cases}\nwhere t_i is given point then\np_{n-1}(t) = y_1l_1(t) + y_2l_2(t) + ··· + y_nl_n(t)\nAlso we can define l_j as\nl_j(t) = \\prod_{i\\neq j}^n \\frac{t - t_i}{t_j - t_i}\nCodes\nLagrange interpolation\nimport numpy as np\n \ndef lagrange(t_vals, f_vals, t):\n\t&quot;&quot;&quot;\n\t\t`t_vals`: argument values used to construct the interpolants.\n\t\t`f_vals`: function values used to construct the interpolants.\n\t\t`t`: Values at which to evaluate the interpolant\n\t&quot;&quot;&quot;\n\tm = len(t_vals)\n    res = np.empty_like(t)\n    for i in range(len(t)):\n        res[i] = 0\n        for j in range(m):\n            sum_up = 1\n            sum_down = 1\n            for k in range(m):\n                if k != j:\n                    sum_up *= t[i] - t_vals[k]\n                    sum_down *= t_vals[j] - t_vals[k]\n            res[i] += sum_up * f_vals[j] / sum_down\n\t\t\n    return res\n \nFast Lagrange interpolation\nimport numpy as np\n \ndef update_w(w, old_m, m):\n\tfor j in range(old_m):\n        for k in range(old_m, m):\n            if j != k:\n                w[j] = w[j] / (t_vals[j] - t_vals[k])\n    for j in range(old_m, m):\n        w[j] = 1\n        for k in range(m):\n            if j != k:\n                w[j] *= t_vals[j] - t_vals[k]\n        w[j] = 1 / w[j]\n    return w\n \ndef improved_lagrange(t_vals, f_vals, t):\n\tm = len(t_vals)\n    res = np.empty_like(t)\n    \n    for i in range(len(t)):\n        w = np.empty_like(f_vals)\n\t\t\n        w = update_w(w, 0, m)\n\t\t\n        right = 1\n        for j in range(0, m):\n            right *= t[i] - t_vals[j]\n        s = 0\n        for j in range(m):\n            s += f_vals[j] * w[j] / (t[i] - t_vals[j])\n        res[i] = right * s\n    return res\n \nbarycentric Lagrange interpolation\nimport numpy as np\n \ndef update_w(w, old_m, m):\n\tfor j in range(old_m):\n        for k in range(old_m, m):\n            if j != k:\n                w[j] = w[j] / (t_vals[j] - t_vals[k])\n    for j in range(old_m, m):\n        w[j] = 1\n        for k in range(m):\n            if j != k:\n                w[j] *= t_vals[j] - t_vals[k]\n        w[j] = 1 / w[j]\n    return w\n \ndef barycentric(t_vals, f_vals, t):\n    res = np.empty_like(t)\n    m = len(t_vals)\n    \n    for i in range(len(t)):\n        w = np.empty_like(f_vals)\n        w = update_w(w, 0, m)\n        \n        s_up = 0\n        s_down = 0\n        for j in range(m):\n            s_up += w[j] * f_vals[j] / (t[i] - t_vals[j])\n            s_down += w[j] / (t[i] - t_vals[j])\n        res[i] = s_up / s_down\n\t\t\n    return res\n \nLagrange with Newton table\nimport numpy as np\n \ndef lagrange(t_vals, f_vals, t):\n    def newton_table(x, y, m):\n        x = np.copy(x[:m])\n        a = np.copy(y[:m])\n        for k in range(1, m):\n            a[k:m] = (a[k:m] - a[k - 1]) / (x[k:m] - x[k - 1])\n        return a\n\t\n    res = np.empty_like(t)\n    m = len(t_vals)\n    for i in range(len(t)):\n        tab = newton_table(t_vals, f_vals, m)\n\t\t    \n        right = np.empty_like(t_vals)\n        right[0] = 1\n        for iii in range(1, m):\n\t        right[iii] = right[iii - 1] * (t[i] - t_vals[iii - 1])\n        res[i] = tab @ right[:m].T\n\t\t\n    return res\n "},"NumericalAnalysis/CH7/Legendre-Polynomials":{"slug":"NumericalAnalysis/CH7/Legendre-Polynomials","filePath":"NumericalAnalysis/CH7/Legendre Polynomials.md","title":"Legendre Polynomials","links":[],"tags":[],"content":"Concept\nstarted with P_0 = 1 and P_1 = x,\nP_{k + 1}(x) = \\frac{2k + 1}{k + 1}tP_k(t) - \\frac{k}{k + 1}P_{k - 1}(t)\n\nWeight\nw(x) = 1"},"NumericalAnalysis/CH7/Runge's-Phenomenon":{"slug":"NumericalAnalysis/CH7/Runge's-Phenomenon","filePath":"NumericalAnalysis/CH7/Runge's Phenomenon.md","title":"Runge's Phenomenon","links":["NumericalAnalysis/CH7/Chebyshev-interpolation"],"tags":[],"content":"Definition\nInterpolation sometime will has significant noise near the edge of range.\nWe can fix it by using Chebyshev nodes, where more nodes distributed near edge, where is Chebyshev interpolation."},"NumericalAnalysis/CH7/Splines":{"slug":"NumericalAnalysis/CH7/Splines","filePath":"NumericalAnalysis/CH7/Splines.md","title":"Splines","links":["NumericalAnalysis/CH7/Splines"],"tags":[],"content":"Concept\nSplines is a piece-wise polynomial\ns(x)\\vert_{\\Omega_j} = a_j + b_jx + c_jx^2 + d_jx^3 + \\dots\nwhere \\Omega_j is j-th domain, every domain has their own function."},"NumericalAnalysis/CH7/basis-function":{"slug":"NumericalAnalysis/CH7/basis-function","filePath":"NumericalAnalysis/CH7/basis function.md","title":"basis function","links":["NumericalAnalysis/CH3/Vandermonde-matrix"],"tags":[],"content":"Concept\nInterpolating function f can be define as linear combination of basis functions\nf(t) = \\sum_{j=1}^n x_j\\phi_j(t)\nFourier basis\ncomprising sines and coses. For example a set of basis functions with n=2k in [0, 2\\pi]:\n\\begin{bmatrix}\n\\phi_1(t) &amp; \\phi_2(t) &amp; \\cdots &amp; \\phi_n(t)\n\\end{bmatrix}\n\\begin{bmatrix}\n\\cos(0) &amp; \\cos(1t) &amp; \\cdots &amp; \\cos(kt) &amp; \\sin(t) &amp; \\cos(2t) &amp; \\cdots &amp; \\cos((k-1)t)\n\\end{bmatrix}\nMonomial basis (bad)\n\\phi_j(t) = t^{j - 1}\nspecifically for this one, we need to use Vandermonde matrix to solve. It require O(n^3) to solve. Considering the bad performance and unstable, we should use other instead.\nNewton Basis\n\\phi_j(x) = \\prod^{j-1}_{i=0}(x-x_i)"},"NumericalAnalysis/CH7/index":{"slug":"NumericalAnalysis/CH7/index","filePath":"NumericalAnalysis/CH7/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH7/orthogonal-polynomial-interpolation":{"slug":"NumericalAnalysis/CH7/orthogonal-polynomial-interpolation","filePath":"NumericalAnalysis/CH7/orthogonal polynomial interpolation.md","title":"orthogonal polynomial interpolation","links":[],"tags":[],"content":"Concept\nbasically in order to use orthogonal polynomial for interpolation, we setup the orthogonal polynomial coefficient matrix with selected polynomials and solve it for coefficients.\nP_0(x_0) &amp; P_1(x_0) &amp; \\cdots &amp; P_{n-1}(x_0) \\\\  \nP_0(x_1) &amp; P_1(x_1) &amp; \\cdots &amp; P_{n-1}(x_1) \\\\  \n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\  \nP_0(x_{n-1}) &amp; P_1(x_{n-1}) &amp; \\cdots &amp; P_{n-1}(x_{n-1})  \n\\end{bmatrix}$$\nA\\vec c = \\vec b\nOr we can use\np_{n-1}(x) = \\sum_{j=1}c_jP_j(x)\nwhere $P$ is the selected polynomial. Then\nc_j = \\frac{(f, P_j)w}{(P_j, P_j)w} = \\frac{\\int{-1}^{1}w(x)f(x)P_j(x)dx}{\\int{-1}^{1}w(x)P_j^2(x)dx}=\\frac{\\int_{-1}^{1}w(x)f(x)P_j(x)dx}{||P_j||^2}\nwhere $(f, P_j)_w$ is [[weighted inner-product]]. \n\n![[Pasted image 20250424172555.png]]\n\n## interpolation error\nf- p = \\frac{f^{(n)}(\\xi)}{n!}q(x)\nwhere\nq(x) = (x - x_1)(x - x_2) \\dots(x - x_n)\n## Crude Error\nassume $t_1 &lt; t_2 &lt; \\cdots &lt; t_n$ we have\n\\max|f(t) - p_{n-1}(t)| \\leq \\frac{Mh^n}{4n}\nwhere $h = \\max_i \\Delta t_i$ and $M$ is constant with $|f^{(n)}| \\leq M$.\n## Code\n```python\nimport numpy as np\n\ndef T(j, x):\n    &quot;&quot;&quot;Compute Chebyshev polynomial T_j(x) = cos(j * arccos(x))&quot;&quot;&quot;\n    return np.cos(j * np.arccos(x))\n\ndef chebyshev_interp(f, n, t):\n    &quot;&quot;&quot;\n    Use Chebyshev polynomials T_j(x) and Gauss-Chebyshev quadrature\n    to interpolate function f on interval [-1, 1].\n    \n    f: function to interpolate\n    n: number of basis functions\n    t: array of evaluation points\n    &quot;&quot;&quot;\n    # chebyshev_nodes\n    x_nodes = np.empty(n)\n    for i in range(n):\n        x_nodes[i] = np.cos((2 * i + 1) * np.pi / (2 * n))\n    \n    f_vals = np.empty(n)\n    for i in range(n):\n        f_vals[i] = f(x_nodes[i])\n    \n    coeffs = np.empty(n)\n    for j in range(n):\n        num = 0\n        denom = 0\n        for i in range(n):\n            Tij = T(j, x_nodes[i])\n            num += f_vals[i] * Tij\n            denom += Tij * Tij\n        num *= np.pi / n\n        denom *= np.pi / n\n        coeffs[j] = num / denom\n    \n    res = np.empty_like(t)\n    for i in range(len(t)):\n        s = 0\n        for j in range(n):\n            s += coeffs[j] * T(j, t[i])\n        res[i] = s\n    \n    return res\n\n```"},"NumericalAnalysis/CH7/weighted-inner-product":{"slug":"NumericalAnalysis/CH7/weighted-inner-product","filePath":"NumericalAnalysis/CH7/weighted inner-product.md","title":"weighted inner-product","links":[],"tags":[],"content":"(f,g)_w = \\int^1_{-1} w(x)f(x)g(x)dx\nand associated norm\n||f||_w = \\sqrt{(f,f)_w}"},"NumericalAnalysis/CH8/Gaussian-Quadrature":{"slug":"NumericalAnalysis/CH8/Gaussian-Quadrature","filePath":"NumericalAnalysis/CH8/Gaussian Quadrature.md","title":"Gaussian Quadrature","links":[],"tags":[],"content":"implementation\nevaluation\nfirst we need to transform from [-1,1] to [a,b] which required to update points and weights\nx_i = \\frac{b-a}{2} \\xi_i + \\frac{a+b}{2}\nwhere x_i is the new mapped point and \\xi_i is the old one\nw_i = \\frac{b-a}{2}w_i\nI = \\int^b_a f dx = \\sum\\frac{b-a}{2}w_if(x_i)\nexact for n node\nup to 2n - 1 degree"},"NumericalAnalysis/CH8/Newton-Cotes-Quadrature":{"slug":"NumericalAnalysis/CH8/Newton-Cotes-Quadrature","filePath":"NumericalAnalysis/CH8/Newton-Cotes Quadrature.md","title":"Newton-Cotes Quadrature","links":[],"tags":[],"content":"implementation rules\n\nMidpoint rule M(f) = (b-a)f(\\frac{a + b}{2})\nTrapezoidal rule T(f) = \\frac{b-a}{2}f(f(a) + f(b))\nSimpson’s rule S(f) = \\frac{b-a}{6}(f(a) + 5f(m) + f(b))\n\nexact for n node\nup to n degree"},"NumericalAnalysis/CH8/Quadrature":{"slug":"NumericalAnalysis/CH8/Quadrature","filePath":"NumericalAnalysis/CH8/Quadrature.md","title":"Quadrature","links":["NumericalAnalysis/CH8/Newton-Cotes-Quadrature","NumericalAnalysis/CH8/Gaussian-Quadrature"],"tags":[],"content":"Concept\nbasically\n\\int^b_a f(x) = \\sum_i^n w_i(x)f(x_i)\nstability upper bound\n|\\Delta Q| \\leq ||w||_1 \\max|\\Delta f_i|\nImplementation\nNewton-Cotes Quadrature use uniformly spaced nodes, Gaussian Quadrature use optimal picked nodes"},"NumericalAnalysis/CH8/Richardson-Extrapolation":{"slug":"NumericalAnalysis/CH8/Richardson-Extrapolation","filePath":"NumericalAnalysis/CH8/Richardson Extrapolation.md","title":"Richardson Extrapolation","links":[],"tags":[],"content":"Concept\nCombing approximation function with different step to reduce the most significant error item.\nExample\n\\tilde F(h) is approximation to F where \\tilde F(h) = F + c_1h^7 + O(h^9) and we want to reduce the h^7 error term.\nWe can use \\hat F(h) = \\alpha \\tilde F(h) + \\beta \\tilde F(2h) as new approximation where\n\n\\alpha + \\beta = 1 hold, because we need at least \\hat F(h) \\approx F hold to keep accuracy, other wise it will has term with (1 + \\varepsilon)F\nc_1(\\alpha h^7 + \\beta (2h)^7) = 0 to reduce the term\n"},"NumericalAnalysis/CH8/basic-difference":{"slug":"NumericalAnalysis/CH8/basic-difference","filePath":"NumericalAnalysis/CH8/basic difference.md","title":"basic difference","links":[],"tags":[],"content":"General\nf&#039;(x) \\approx c_1f(x + h) + c_2f(x) + c_3f(x - h)\nby Taylor expansion\nf(x + h) = f(x) + hf&#039;(x) + \\frac{h^2}{2!}f&#039;&#039;(x) + \\cdots + \\frac{h^n}{n!}f^{(n)}(x)\nf(x - h) = f(x) - hf&#039;(x) + \\frac{h^2}{2!}f&#039;&#039;(x) - \\cdots + \\frac{h^n}{n!}f^{(n)}(x)\nForward difference\nf&#039;(x) \\approx \\frac{f(x + h) - f(x)}{h} = f&#039;(x) + Ch + O(h^2)\nCenter difference\nf&#039;(x) \\approx \\frac{f(x + h) - f(x - h)}{2h} = f&#039;(x) + Ch^2 + O(h^4)"},"NumericalAnalysis/CH8/index":{"slug":"NumericalAnalysis/CH8/index","filePath":"NumericalAnalysis/CH8/index.md","title":"index","links":[],"tags":[],"content":""},"NumericalAnalysis/CH9/Backward-Euler":{"slug":"NumericalAnalysis/CH9/Backward-Euler","filePath":"NumericalAnalysis/CH9/Backward Euler.md","title":"Backward Euler","links":["NumericalAnalysis/CH9/Forward-Euler"],"tags":[],"content":"Definition\na implicit method\ny_{k+1} = y_k + \\Delta tf(t_{k+1},y_{k+1})\nStability\nunlike Forward Euler, It’s stable under any step h &gt; 0."},"NumericalAnalysis/CH9/Forward-Euler":{"slug":"NumericalAnalysis/CH9/Forward-Euler","filePath":"NumericalAnalysis/CH9/Forward Euler.md","title":"Forward Euler","links":[],"tags":[],"content":"Definition\ny_{k+1} = y_k + \\Delta tf(t_k,y_k)\nwhen it’s linear, we can say\ny_{k + 1} = y_k + \\Delta t\\lambda y_k = y_k(1 + \\Delta t\\lambda)\nStability\nIt’s stable only if all \\lambda: |1 + h\\lambda_i| \\leq 1"},"NumericalAnalysis/CH9/Runge-Kutta-method":{"slug":"NumericalAnalysis/CH9/Runge-Kutta-method","filePath":"NumericalAnalysis/CH9/Runge-Kutta method.md","title":"Runge-Kutta method","links":[],"tags":[],"content":"Definition\ncan be either implicit or explicit\ny_{k + 1} = y_k + \\int^{t_{k+1}}_{t_k} f(t,y)dt\nexplicit RK4\nthe most common Runge-Kutta scheme, 4-th order\ny_{k + 1} = y_k + \\frac{h_k}{6}(k_1 + k_2 + k_3 + k_4)"},"NumericalAnalysis/CH9/Trapezoidal-Rule":{"slug":"NumericalAnalysis/CH9/Trapezoidal-Rule","filePath":"NumericalAnalysis/CH9/Trapezoidal Rule.md","title":"Trapezoidal Rule","links":[],"tags":[],"content":"Definition\na implicit method\ny_{k + 1} = y_k + h\\frac{f(t_k,y_k) + f(t_{k + 1}, y_{k + 1})}{2}\nStability\nIt’s sable if\n|\\frac{1 + \\frac{h\\lambda}{2}}{1-\\frac{h\\lambda}{2}}| \\leq 1\nfor every \\lambda as Eigenvalue"},"NumericalAnalysis/CH9/index":{"slug":"NumericalAnalysis/CH9/index","filePath":"NumericalAnalysis/CH9/index.md","title":"index","links":[],"tags":[],"content":""},"RealAnalysis/integrableAndDiff/Darboux-integral":{"slug":"RealAnalysis/integrableAndDiff/Darboux-integral","filePath":"RealAnalysis/integrableAndDiff/Darboux integral.md","title":"Darboux integral","links":["RealAnalysis/integrableAndDiff/partition","RealAnalysis/sequences/bounded","RealAnalysis/sequences/convergent","RealAnalysis/integrableAndDiff/Riemann-integral"],"tags":[],"content":"Def 27.3\nfor partition \\delta \\subset [a, b]\n\n\\Delta_j = x_{j - 1} - x_j\n\\inf_I f = \\inf_{x \\in I} f(x) and \\sup_I f = \\sup_{x \\in I} f(x)\n\\underline{\\sum}_\\sigma = \\sum^{n - 1}_{j = 0} \\Delta_j \\inf_{I_j}f and \\overline{\\sum}_\\sigma = \\sum^{n - 1}_{j = 0} \\Delta_j \\sup_{I_j}f where I_j = [x_j, x_{j + i}]\n\nThm 29.1\nThen we can take I=\\lim_k \\overline{\\sum}_{\\tau_k} f  and \\tau_k = \\{x^k_j \\mid 0 \\leq j \\leq 2^k\\} where \\tau_k \\subset \\tau_{k + 1}.\nIndeed we can say \\overline{\\sum}_{\\tau_{k + 1}} \\leq \\overline{\\sum}_{\\tau_k} and (\\overline{\\sum}_{\\tau_k}) is bounded below and monotone decreasing, which imply it is convergent.\nRemarks:\n\nx^k_j means j-th element in x^k partition\nx^k partition means we evenly split it into 2^k elements\n\nLemma 28.2\nFor two partition \\sigma \\subset \\tau\n\\underline{\\sum}_\\sigma f \\leq \\underline{\\sum}_\\tau f \\leq R_\\tau(R, \\xi) \\leq \\overline{\\sum}_\\tau \\leq \\overline{\\sum}_\\sigma f\nwhere R is Riemann integral\nBasically, if the largest sub-interval is larger, we will get more inaccurate result.\nLemma 28.3\nFor two partition \\sigma \\subset \\tau , M = \\sup(f(x) - f(y)) which is the worst difference where a \\leq x, y \\leq b, we can say \\overline{\\sum}_\\tau \\leq \\overline{\\sum}_\\sigma f \\leq \\overline{\\sum}_\\tau + |\\tau| M(\\#\\tau - \\#\\sigma) where \\# is the number of that set."},"RealAnalysis/integrableAndDiff/Power-theory":{"slug":"RealAnalysis/integrableAndDiff/Power-theory","filePath":"RealAnalysis/integrableAndDiff/Power theory.md","title":"Power theory","links":["RealAnalysis/sequences/uniform-limit","RealAnalysis/sequences/continue","RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/convergent","RealAnalysis/integrableAndDiff/differentiable","RealAnalysis/integrableAndDiff/integrable"],"tags":[],"content":"Thm 37.1\nFor \\sum a_kx^k = f(x) we can prove\n\nf&#039;(x) = \\sum k a_k x^{k -1}\n\\int f(t) dt = \\sum \\frac{a_kx^{k+1}}{k+1}\n\nThm 37.2\nFor uniform limit \\lim f_n&#039; = g where g is continue and \\lim f_n = f point-wise, we can say f&#039;=g.\nThm 37.3\nFor \\lim f_n = f is uniform limit and \\lim f_n = f in Metric Space \\Longrightarrow \\lim \\int f_n(t) dt = \\int f(t) dt is uniform limit.\nDef 37.4 Cauchy-Hadamard theorem\nR = \\left[ \\lim_n \\sup \\sqrt[n]{|a_n|} \\right]^{-1}\nProps 37.5\n(\\forall r &lt; R) f_n(x) = \\sum^n_{k = 0} a_kx^k are uniformly Cauchy on C[-r,r]\nCor 37.6\n(\\forall r &lt; R) f_n uniformly converge to f on [-r, r] and \\sum a_kx^k convergent.\nLemma 37.7\n\\lim_n\\sup \\sqrt[n]{|a_n|} = \\lim_n\\sup\\sqrt[n]{(k+1)|a_{n+1}|}=\\lim_n\\sup\\sqrt[n]{\\frac{|a_{n-1}|}{k}}\nThm 37.8\nlet (a_n) and R=\\left( \\lim_n\\sup\\sqrt[n]{|a_n|}\\right)^{-1}\nThen on (-R, R), f(x) = \\sum_k a_kx^k is\n\nWell defined\ncontinue\ndifferentiable\nintegrable\n\nDef 38.1\n\\frac{1}{1-x} = \\sum^\\infty_{k=0} x^k\nRemark 38.2\n\nRadius of convergent comes from complex inequality.\n\nProp 38.3\n\\lim_k\\sup \\sqrt[k]{k} = 1"},"RealAnalysis/integrableAndDiff/Riemann-integral":{"slug":"RealAnalysis/integrableAndDiff/Riemann-integral","filePath":"RealAnalysis/integrableAndDiff/Riemann integral.md","title":"Riemann integral","links":["RealAnalysis/integrableAndDiff/Darboux-integral","RealAnalysis/integrableAndDiff/partition"],"tags":[],"content":"Def Def 28.1\nR(f, \\xi) = \\sum_j^{||\\xi||}\\Delta_j f(\\xi_j)\n\\sup R_\\sigma(f, \\xi) = \\overline{\\sum}_{\\sigma} f\nWhere \\overline{\\sum} is in Darboux upper sum\nDef 30.1\n\\lim_{n\\longrightarrow\\infty} \\frac{1}{n} \\sum^n_{j=1} f(a + j\\frac{b-a}{n}) = \\int^b_a f(x) dx\nOr\n\\lim_{n \\to \\infty} \\sum^{n-1}_{i=0} f(x_i)(x_{i+1}-x_i) = \\int^b_a f(x)dx\nBasically we just need the parition be small enough.\nLemma 30.2\n\nR(f + \\lambda g, \\xi) = R(f, \\xi) + \\lambda R(g, \\xi)\nfor [a, b] = [a, c] \\cup [c, b]\n\n\\sigma = \\sigma_{[a, c]} \\cup \\sigma_{[c, d]} where it’s partition\n\\Delta_{\\sigma_{[a,c] \\cup \\{c\\}}} + \\Delta_{\\sigma_{[c, b] \\cup \\{c\\}}} \\leq 2 \\Delta_\\sigma\n\n\n\nDef 10.2\n\\Delta_\\sigma(f) = \\sum (x_{j + 1} - x_j)(\\sup f - \\inf f)"},"RealAnalysis/integrableAndDiff/Trigonometry":{"slug":"RealAnalysis/integrableAndDiff/Trigonometry","filePath":"RealAnalysis/integrableAndDiff/Trigonometry.md","title":"Trigonometry","links":["RealAnalysis/integrableAndDiff/differentiable"],"tags":[],"content":"Basic recall\n\n\\sin&#039;(x) = \\cos(x)\n\\cos&#039;(x) = -\\sin(x)\n\\sin(0) = 0, \\cos(0) = 1\n\\sin(\\frac{\\pi}{2})=1, \\cos(\\frac{\\pi}{2}) = 0\n\\sin(-x)=-\\sin(x)\n\nDef 41.1\n\\pi = \\sup\\{x \\mid \\forall 0 &lt; y &lt; x, \\cos(y) = \\infty\\}\nLemma 41.2\n|\\sin(x)| \\leq |x|\nproved by Thm 32.1 Fundamental Theorem and (\\forall t)\\cos(t)\\leq 1 with\n\\sin(x) - \\sin(0) = \\int^x_0 \\sin&#039;(t)dt = \\int^x_0 \\cos(t)dt \\leq x \\leq |x|\nThen\n-\\sin(x) = \\sin(-x) \\leq |x|\n\\square\nRemark 41.3\n1-\\cos(x) = \\int^x_0 \\sin(t)dt"},"RealAnalysis/integrableAndDiff/differentiable":{"slug":"RealAnalysis/integrableAndDiff/differentiable","filePath":"RealAnalysis/integrableAndDiff/differentiable.md","title":"differentiable","links":["RealAnalysis/integrableAndDiff/differentiable","RealAnalysis/sequences/bounded","RealAnalysis/sequences/Lipschitz","RealAnalysis/integrableAndDiff/integrable","RealAnalysis/sequences/continue"],"tags":[],"content":"Def 31.3\nF is differentiable at x if\n(\\exists a)(\\forall \\varepsilon &gt; 0)(\\exists \\delta)|x - y| &lt; \\delta \\Longrightarrow |\\frac{F(y) - F(x)}{y - x} - a| &lt; \\varepsilon\nwhere we can say a =  F&#039;(x)\nThm 31.4\ndifferentiable f on (a, b) and f&#039;(x) bounded \\Longrightarrow f is Lipschitz\nThm 32.1 Fundamental Theorem\n\n\\int^x_ag&#039;(t)dt = g(x) - c\nif F&#039; is integrable, F(b) - F(a) = \\int^b_a F&#039;(t)dt\n\nThm 32.2 Mean Value Theorem(MVT)\nlet f be continue on [a,b] and f is differentiable on (a, b)\n(\\exists \\xi \\in (a,b)) \\frac{f(b) - f(a)}{b - a} = f&#039;(\\xi)\nRolle’s Lemma\nLet f be continue on [a, b] and differentiable on (a,b)\nf(a) = f(b) \\Longrightarrow (\\exists \\xi) f&#039;(\\xi) = 0\nRules\n\n(f + \\lambda g)&#039; = f&#039; + \\lambda g&#039;\n(f\\circ g)&#039; = (f&#039; \\circ g)g&#039;\n(fg)&#039; = f&#039;g + fg&#039;\nf \\in [a,b] is strict monotone, f^{-1} is continue\n\nThm 40.1 Chain rule\n(f\\circ g)&#039; = f&#039;(g(x))g&#039;(x)\nProp 40.1\nLet g: \\mathbb{R} \\longrightarrow \\mathbb{R}\n\\frac{d}{dy} \\int^b_a g(y,t)dt = \\int^b_a \\frac{d}{dy}g(y, t)dt\nThm 40.2 differential\nH&#039;(a) = \\lim_n \\frac{H(a + \\xi_n) - H(a)}{\\xi_n}\nThm\nInverse of differentiable function is differentiable.\nThm\n\\frac{f(x) - f(x_0)}{g(x)-g(x_0)} = \\frac{f&#039;(\\xi)}{g&#039;(\\xi)}\nx_0 &lt; \\xi &lt; x\nThm\nh&#039; is continue \\iff h is continue and differentiable."},"RealAnalysis/integrableAndDiff/integrable":{"slug":"RealAnalysis/integrableAndDiff/integrable","filePath":"RealAnalysis/integrableAndDiff/integrable.md","title":"integrable","links":["RealAnalysis/sequences/bounded","RealAnalysis/integrableAndDiff/integrable","RealAnalysis/integrableAndDiff/partition","RealAnalysis/integrableAndDiff/mesh","RealAnalysis/sequences/continue","RealAnalysis/sequences/Lipschitz"],"tags":[],"content":"Def 28.5 pg. 228 Darboux integrable\nFor I = [a,b] and f:I \\longrightarrow \\mathbb{R} be a bounded function. Then f is Darboux integrable if (\\exists \\tau)(\\forall \\varepsilon &gt; 0)\\overline{\\sum}_\\tau f - \\underline{\\sum}_\\tau f &lt; \\varepsilon\nDef 28.1 Riemann integrable\n(\\exists I)(\\forall \\varepsilon &gt; 0)(\\exists \\delta)(\\exists \\sigma)(\\exists\\xi) |\\sigma| &lt; \\delta \\Longrightarrow |R(f, \\xi) - I| &lt; \\varepsilon\nwhere \\sigma is a partition, \\delta is the max mesh size, and \\xi is a set with points from every sub-interval in \\sigma, R is Riemann Sum.\nThm 28.4\nDarboux integrable imply Riemann integrable\nThm 29.5\nf continue imply f integrable\nLemma 10.4\nlet \\Phi be Lipschitz and f integrable, we have \\Phi \\circ f integrable.\nAlso \\Delta_\\sigma(\\Phi \\circ f) \\leq L \\Delta_\\sigma(f)\nThm 31.1 Lebesgue’s Criterion for Riemann Integrability\nf is bounded and \\exists k \\subseteq [a,b], the following are equivalent:\n\nf is Riemann integrable on k\nthe set of discontinuities of f on k has Lebesgue measure zero\nBasically, it means, f is only discontinue in measure zero set, and continue otherwise.\n\nThm 31.2\n\\varphi is continue, f integrable \\Longrightarrow \\varphi \\circ f integrable\nOH 35.1\nThm A\n\\varphi Lipschitz and f integrable \\Longrightarrow \\varphi \\circ f integrable\nThm B\n\\varphi continue and f integrable \\Longrightarrow \\varphi \\circ f integrable\nProof\nBy Thm C, we can say there exists S has Lebesgue Zero Measure.\nThm C\nf integrable \\iff (\\exists S) has Lebesgue Zero Measure"},"RealAnalysis/integrableAndDiff/integral":{"slug":"RealAnalysis/integrableAndDiff/integral","filePath":"RealAnalysis/integrableAndDiff/integral.md","title":"integral","links":["RealAnalysis/integrableAndDiff/integrable","RealAnalysis/sequences/Lipschitz"],"tags":[],"content":"Def 29.3\n\\int^b_a f(x) dx = I = \\lim_k \\overline{\\sum}_{\\tau_k} f= \\lim R(f, \\xi)\nif f is integrable\nProp 29.4\n\\forall \\sigma\n\n\\underline{\\sum}_\\sigma f \\leq \\int ^b_a f(x) dx \\leq \\overline{\\sum}_\\sigma f\n| \\int^b_a f(x) dx | \\leq \\overline{\\sum}_\\sigma |f|\n|\\int^b_a f(x) dx| \\leq \\int_a^b |f(x)| dx\n\nThm 30.3\nfor f, g is integrable\n\n\\int^b_a (f(t) + \\lambda g(t))dt = \\int^b_a f(t)dt + \\lambda\\int^b_a g(t)dt\n\\int^b_af(t)dt = \\int^c_af(t)dt + \\int^b_c f(t)dt\n\nThm 30.5\n|\\int^b_a f dt| \\leq \\int^b_a |f| dt\nproved by considering \\Phi(y) = |y| as a Lipschitz function.\nDef 31.5\nBy Oresme\n\\int^1_0 x^mdx = \\frac{1}{m+1}\nProp 41.3\n\\int^b_a f(s) ds \\leq (b-a)\\sup_s f(s)"},"RealAnalysis/integrableAndDiff/mesh":{"slug":"RealAnalysis/integrableAndDiff/mesh","filePath":"RealAnalysis/integrableAndDiff/mesh.md","title":"mesh","links":["RealAnalysis/integrableAndDiff/mesh","RealAnalysis/integrableAndDiff/partition"],"tags":[],"content":"Def pg. 200\nThe mesh (or norm) of partition is defined by\n|| \\mathcal{P} || = \\max\\{x_1 - x_0, x_2 - x_1, \\dots, x_n - x_{n - 1}\\}\nwhich is the largest length of all sub-interval"},"RealAnalysis/integrableAndDiff/partition":{"slug":"RealAnalysis/integrableAndDiff/partition","filePath":"RealAnalysis/integrableAndDiff/partition.md","title":"partition","links":["RealAnalysis/sequences/closed","RealAnalysis/sequences/bounded","RealAnalysis/sequences/interval","RealAnalysis/integrableAndDiff/partition"],"tags":[],"content":"Def pg. 199\nFor a closed bounded interval [a, b], we can say \\mathcal{P}=\\{x_{i-1}, x_i\\}^n_{i=1}  is it’s partition where x_0 = a and x_n = b. By definition, we can say \\mathcal{P} is the set of points that split the [a, b] into many non-overlapped sub-interval."},"RealAnalysis/sequences/Cauchy-sequence":{"slug":"RealAnalysis/sequences/Cauchy-sequence","filePath":"RealAnalysis/sequences/Cauchy sequence.md","title":"Cauchy sequence","links":["RealAnalysis/sequences/convergent","RealAnalysis/sequences/complete","RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/Cauchy-sequence"],"tags":[],"content":"Def 14.1\n(x_{n}) is called Cauchy if\n(\\forall \\varepsilon &gt; 0)(\\exists n_0) (\\forall n, m &gt; n_0) d(x_n, x_m) &lt; \\varepsilon \nThm\nAll convergent sequence in complete Metric Space is Cauchy sequence."},"RealAnalysis/sequences/Delta-ball":{"slug":"RealAnalysis/sequences/Delta-ball","filePath":"RealAnalysis/sequences/Delta-ball.md","title":"Delta-ball","links":[],"tags":[],"content":"Def 13.2\nB_\\delta(x) = \\{y \\mid d(x, y) \\leq \\delta \\}\nclosed \\delta ball around x\nB_\\delta(x) = \\{y \\mid d(x, y) &lt; \\delta \\}\nopen \\delta ball around x"},"RealAnalysis/sequences/Lipschitz":{"slug":"RealAnalysis/sequences/Lipschitz","filePath":"RealAnalysis/sequences/Lipschitz.md","title":"Lipschitz","links":["RealAnalysis/sequences/Lipschitz","RealAnalysis/sequences/uniformly-continue","RealAnalysis/integrableAndDiff/differentiable","RealAnalysis/sequences/continue","RealAnalysis/sequences/bounded","RealAnalysis/integrableAndDiff/integrable"],"tags":[],"content":"Def 17.3\nf:(x, d) \\longrightarrow (y, d&#039;) is called Lipschitz\nif d(f(x), f(y)) \\leq k d(x, y)\nwhere k&gt;0 is called Lipschtz constant\nThm pg.143\nif f: A \\longrightarrow \\mathbb{R} is Lipschitz function, f is uniformly continue in A\nThm 24.1\nThe best(smallest) Lipschitz constant equal to the first order derivative of a differentiable function.\nOH 35.1\nThm D\nfor f continue, f&#039; bounded and f&#039; integrable \\Longrightarrow f Lipschitz\nProof\nby FT:\nf(b) - f(a) = \\int^b_a f&#039;(t)dt \\leq \\overline\\sum\\Delta\\sup f(\\xi) \\leq (b-a)\\sup_kf(\\xi_k)\nThm E\n\\exists f&#039; and f&#039; is bounded \\Longrightarrow f is Lipschitz\nProof\nby MVT\n(\\exists \\xi \\in (a,b)) \\frac{f(b) - f(a)}{b - a} = f&#039;(\\xi)\n\\Longrightarrow f(b) - f(a) = f&#039;(\\xi)(b-a) \\leq (b-a) \\sup f&#039;\nThm\nf uniformly continue \\Longrightarrow f Lipschitz"},"RealAnalysis/sequences/Metric-Space":{"slug":"RealAnalysis/sequences/Metric-Space","filePath":"RealAnalysis/sequences/Metric Space.md","title":"Metric Space","links":[],"tags":[],"content":"Def 13.1\n(X, d) is called metric space\n0) X is set\n\nd: X \\times X \\longrightarrow [0, \\alpha)\n\nd(x, y) = d(y, x)\nd(x, y) = 0 \\iff x = y\nd(x ,y) \\leq d(x, z) + d(z, y)\n\n\n\n3 standard choices of distance function 17.4\n\nd_\\infty(x, y) = \\max_d |x(d) - y(d)|\nd_2(x, y) = \\sqrt{\\sum_d |x(d) - y(d)|^2}\nd_1(x, y) = \\sum_{d=1}^d |x(d) - y(d)|\n"},"RealAnalysis/sequences/bounded":{"slug":"RealAnalysis/sequences/bounded","filePath":"RealAnalysis/sequences/bounded.md","title":"bounded","links":["RealAnalysis/sequences/bounded","RealAnalysis/sequences/Delta-ball"],"tags":[],"content":"Def for R\nFor sequence (x_n), we can say it’s bounded if\n(\\exists M &gt; 0)(\\forall n \\in \\mathbb{N}) |x_n| &lt; M\nDef 23.5\nA set S \\in X is called bounded if there\n(\\exists R &gt; 0)(\\exists x \\in X) S \\subset B_R(x)\nWhere B_R is a Delta-ball.\nDef\nIf we say a function is bounded, we can say\n(\\exists M &gt; 0)(\\forall x)|f(x)| &lt; M"},"RealAnalysis/sequences/closed":{"slug":"RealAnalysis/sequences/closed","filePath":"RealAnalysis/sequences/closed.md","title":"closed","links":["RealAnalysis/sequences/cluster-points","RealAnalysis/sequences/closed","RealAnalysis/sequences/sequentially-Closed"],"tags":[],"content":"Def\nA set is closed if\n\nIt’s complement set is open\nAll cluster points are in the set\n\nProp 15.2\nC \\subset X\nTFAE\n\nC is closed\nC is sequentially Closed\n"},"RealAnalysis/sequences/closure":{"slug":"RealAnalysis/sequences/closure","filePath":"RealAnalysis/sequences/closure.md","title":"closure","links":["RealAnalysis/sequences/closed"],"tags":[],"content":"Def 17.1\n\\bar{S} = \\{\\lim_k x_k \\mid x_k \\in S \\} \\cup S\nis closure of set S, it has following properties\n\n\\bar{S} is closed\n\\bar{S} is smallest closed set which contain set S\n\nDef 18.1\nS is dense in \\bar{S}\n(\\forall y \\in \\bar{S})(\\exists x \\in S)(\\forall \\varepsilon &gt; 0) d(x, y) &lt; \\varepsilon"},"RealAnalysis/sequences/cluster-points":{"slug":"RealAnalysis/sequences/cluster-points","filePath":"RealAnalysis/sequences/cluster points.md","title":"cluster points","links":["RealAnalysis/sequences/cluster-points","RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/convergent"],"tags":[],"content":"Def pg.104\nFor set A \\subset \\mathbb{R}, if c \\in \\mathbb{R} is cluster point, we can say\n(\\forall \\varepsilon &gt; 0)(\\exists x \\neq c \\in A) |x - c| &lt; \\varepsilon\nBasically, for common Metric Space, it’s equivalent to the limit point."},"RealAnalysis/sequences/compact":{"slug":"RealAnalysis/sequences/compact","filePath":"RealAnalysis/sequences/compact.md","title":"compact","links":["RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/open-cover","RealAnalysis/sequences/subcover","RealAnalysis/sequences/compact","RealAnalysis/sequences/totally-bounded","RealAnalysis/sequences/closed","RealAnalysis/sequences/sequentially-compact","RealAnalysis/sequences/continue","RealAnalysis/sequences/sequentially-continue","RealAnalysis/sequences/bounded"],"tags":[],"content":"Def pg.346 21.5\nA Metric Space (S, d) is compact if each open cover of S has finite subcover.\nLemma 21.6\nS is compact \\Longrightarrow totally bounded\nThm 21.7\nS \\subset (X, d) TFAE\n\nS compact\nS is closed, totally bounded\nS is sequentially compact\n\nThm 28 OH\nFor C \\subset X compact and continue function f: X \\longrightarrow \\mathbb{R}\n\n\\sup_{x \\in C} f(x) &lt; \\infty\n(\\exists x_0 \\in X) \\sup_{x \\in C} f(x) = f(x_0) according to every continue function reach it’s maximum.\n\nHeine–Borel theorem\nS \\subset \\mathbb{R}^d\n\nS is compact, where every open cover has finite subcover\nS is closed and bounded\n\nThm\nInverse image of compact set is compact."},"RealAnalysis/sequences/complete":{"slug":"RealAnalysis/sequences/complete","filePath":"RealAnalysis/sequences/complete.md","title":"complete","links":["RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/complete","RealAnalysis/sequences/Cauchy-sequence","RealAnalysis/sequences/convergent","RealAnalysis/sequences/closed"],"tags":[],"content":"Def 14.2\nA metric space is complete if every Cauchy sequence in X is convergent to some points in the set X.\nThm 15.1\nFor X(C, d) a Metric Space\nTFAE\n\nC is closed\n(C, d) is complete\n\nThm 17.6 Existence of completion\nFor (X, d) is a Metric Space, then there exists a complete Metric Space \\underline{Y} and \\phi: X \\longrightarrow \\underline{Y}\n\nd_\\underline{Y}(\\phi(x), \\phi(y)) = d(x, y)\n\\phi(x) is dense in \\underline{Y}\n\nDef 18.3\ncompletion is unique(too complex to prove)"},"RealAnalysis/sequences/connected":{"slug":"RealAnalysis/sequences/connected","filePath":"RealAnalysis/sequences/connected.md","title":"connected","links":["RealAnalysis/sequences/connected","RealAnalysis/sequences/open-set","RealAnalysis/sequences/interval","RealAnalysis/sequences/continue"],"tags":[],"content":"Def 19.1\nA set S is not connected if exists open set O_1, O_2 such that\n\nS \\subseteq O_1 \\cup O_2\nS \\land O_1 \\neq \\emptyset\nS\\land O_2 \\neq \\emptyset\nS \\cap O_1 \\cap O_2 = \\emptyset\n\nThm 19.2\nS \\subset \\mathbb{R} is connected \\iff S is interval\nThm 19.3\nfor f: X \\longrightarrow Y continue, S connected \\Longrightarrow f(S) connected\nThm Intermediate value 19.4\nf: [a, b] \\longrightarrow \\mathbb{R} be continue, y \\in [f(a), f(b)] \\Longrightarrow (\\exists a \\leq t \\leq b)p(t) = y"},"RealAnalysis/sequences/continue":{"slug":"RealAnalysis/sequences/continue","filePath":"RealAnalysis/sequences/continue.md","title":"continue","links":["RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/continue","RealAnalysis/sequences/uniform-limit","RealAnalysis/sequences/sequentially-continue"],"tags":[],"content":"Def 16.1\nf is continue imply\nd(x, x&#039;) &lt; \\varepsilon \\Longrightarrow d(f(x), f(x&#039;)) &lt; \\varepsilon\nThm 16.2\nf is continue at x \\iff (\\lim_n x_n = x \\Longrightarrow \\lim_n f(x_n) = f(x))\nThm 16.3 17.2\nf: x \\longrightarrow y TFAE\n\nf is continuous\nit’s inverse image f^{-1}(O) is open for all open image O\n\nThm 17.5\nA Metric Space (X, d) and f, g \\in C(X, \\mathbb{R})\n\nf + g \\in C(X, \\mathbb{R})\nf\\times g \\in C(X, \\mathbb{R})\n\\exists \\phi: \\mathbb{R} \\longrightarrow \\mathbb{R} \\text{ c.t. } \\Longrightarrow \\phi \\circ f \\in C(X, \\mathbb{R})\nwhere C is the set of all continue functions from X to \\mathbb{R}\n\nThm 17.7\nThe uniform limit of continuous functions is continue\nThm\ncontinue \\iff sequentially continue\nThm 35.2\ninverse function of continue function is continue."},"RealAnalysis/sequences/convergent":{"slug":"RealAnalysis/sequences/convergent","filePath":"RealAnalysis/sequences/convergent.md","title":"convergent","links":[],"tags":[],"content":"Def\n\\lim_n(x_n) = x&#039; \\iff (\\forall \\varepsilon &gt; 0)(\\exists n_0)(\\forall n \\geq n_0) |x_n - x&#039;| &lt; \\varepsilon"},"RealAnalysis/sequences/convex-set":{"slug":"RealAnalysis/sequences/convex-set","filePath":"RealAnalysis/sequences/convex set.md","title":"convex set","links":["RealAnalysis/sequences/convex-set","RealAnalysis/sequences/connected"],"tags":[],"content":"Cor 19.7\nconvex set \\Longrightarrow connected"},"RealAnalysis/sequences/interval":{"slug":"RealAnalysis/sequences/interval","filePath":"RealAnalysis/sequences/interval.md","title":"interval","links":["RealAnalysis/sequences/interval"],"tags":[],"content":"five types:\n\nOpen\nClosed\nhalf-open/closed\ninfinity open\ninfinity closed\n\nDef pg.47\nIf (\\forall x,y \\in S) x &gt; y \\Longrightarrow [x, y] \\subseteq S is true, then S is interval."},"RealAnalysis/sequences/neighborhood":{"slug":"RealAnalysis/sequences/neighborhood","filePath":"RealAnalysis/sequences/neighborhood.md","title":"neighborhood","links":[],"tags":[],"content":"Def pg. 326\nA neighborhood of a point x \\in \\mathbb{R} is any set V that contains an \\varepsilon-neighborhood V_\\varepsilon(x) := (x - \\varepsilon, x + \\varepsilon) of x for some \\varepsilon &gt; 0."},"RealAnalysis/sequences/open-cover":{"slug":"RealAnalysis/sequences/open-cover","filePath":"RealAnalysis/sequences/open cover.md","title":"open cover","links":["RealAnalysis/sequences/open-cover"],"tags":[],"content":"Def pg. 333\nlet A \\subset \\mathbb{R}, an open cover of A is a collection \\mathcal{G} = \\{G_\\alpha\\} where all G_\\alpha is open, such that\nA \\subseteq \\bigcup_\\alpha G_\\alpha"},"RealAnalysis/sequences/open-set":{"slug":"RealAnalysis/sequences/open-set","filePath":"RealAnalysis/sequences/open set.md","title":"open set","links":["RealAnalysis/sequences/open-set","RealAnalysis/sequences/neighborhood","RealAnalysis/sequences/closed"],"tags":[],"content":"Def 16.4\n\\emptyset is open\nDef pg. 327\n\nA subset G of \\mathbb{R} is open set in \\mathbb{R} if for each x \\in G there exits a neighborhood V of x that V \\subseteq G.\nA subset F of \\mathbb{R} is closed in \\mathbb{R} if the complement C(F) := \\mathbb{R} \\backslash F is open in \\mathbb{R}\n"},"RealAnalysis/sequences/path-connected":{"slug":"RealAnalysis/sequences/path-connected","filePath":"RealAnalysis/sequences/path connected.md","title":"path connected","links":["RealAnalysis/sequences/path-connected","RealAnalysis/sequences/continue","RealAnalysis/sequences/connected"],"tags":[],"content":"Def Path connected 19.5\nS \\subset X is path connected if x \\neq y \\in S (\\exists \\gamma: [0, 1] \\longrightarrow S) \\gamma continue and \\gamma(0) = x and \\gamma(1) = y\nThm 19.6\npath connected \\Longrightarrow connected\nExample\nwe can prove it’s path connected by manually making a path. e.g.\nf as upper bound and g as lower bound, we can make 3 paths and concat them.\n\ngo up to f from start x coordination\ngo through f to end x coordination\ngo down to end y coordination\n"},"RealAnalysis/sequences/sequentially-Closed":{"slug":"RealAnalysis/sequences/sequentially-Closed","filePath":"RealAnalysis/sequences/sequentially Closed.md","title":"sequentially Closed","links":["RealAnalysis/sequences/convergent"],"tags":[],"content":"Def 8.5\n[a, b] is sequentially closed imply\n(\\exists x = \\lim_n (x_n))(a \\leq x_n \\leq b) \\Longrightarrow a \\leq x \\leq b \\Longrightarrow x \\in [a, b]\nBasically, all limits of it’s sub-sequences are in itself."},"RealAnalysis/sequences/sequentially-compact":{"slug":"RealAnalysis/sequences/sequentially-compact","filePath":"RealAnalysis/sequences/sequentially compact.md","title":"sequentially compact","links":["RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/compact","RealAnalysis/sequences/sequentially-compact","RealAnalysis/sequences/closed"],"tags":[],"content":"\nfor common Metric Space, compact \\iff sequentially compact\n\nDef 20.1\nS \\subset (X, d) is called sequentially compact if every sequence has a convergence subsequence with limit in S\nLemma 20.2\nfor compact sequence S\nT \\subseteq S is closed \\Longrightarrow T is sequentially compact\nThm 20.3 21.1\nS \\subseteq \\mathbb{R}^d and sequence S compact or sequentially compact \\iff\n\nS is bounded\nS is closed\n"},"RealAnalysis/sequences/sequentially-continue":{"slug":"RealAnalysis/sequences/sequentially-continue","filePath":"RealAnalysis/sequences/sequentially continue.md","title":"sequentially continue","links":["RealAnalysis/sequences/sequentially-continue"],"tags":[],"content":"Def 7.6\nA function f: I \\longrightarrow \\mathbb{R} is called sequentially continue if\n(\\lim_n x_n = x) \\Longrightarrow (\\lim_n f(x_n) = f(x))\nThm 8.1\nAll sequentially continue function on [a, b] attain it’s maximum"},"RealAnalysis/sequences/subcover":{"slug":"RealAnalysis/sequences/subcover","filePath":"RealAnalysis/sequences/subcover.md","title":"subcover","links":["RealAnalysis/sequences/open-cover","RealAnalysis/sequences/subcover"],"tags":[],"content":"Def pg. 333\n\nThe construction of \\mathcal{G} is in open cover\n\nIf \\mathcal{G}&#039; is a sub-collection of \\mathcal{G} such that it’s union also contain origin set A, then \\mathcal{G}&#039; is called subcover of \\mathcal{G}. If \\mathcal{G}&#039; has consist of finitely many set, we can call \\mathcal{G}&#039; a finite subcover of \\mathcal{G}.\nDef 21.4\nA subcover of open cover is an open cover"},"RealAnalysis/sequences/totally-bounded":{"slug":"RealAnalysis/sequences/totally-bounded","filePath":"RealAnalysis/sequences/totally bounded.md","title":"totally bounded","links":["RealAnalysis/sequences/totally-bounded","RealAnalysis/sequences/Delta-ball","RealAnalysis/sequences/bounded"],"tags":[],"content":"Def 21.1\nS \\subset X is totally bounded if\n(\\forall \\varepsilon &gt; 0)(\\exists m)(\\exists x_1, x_2, \\dots, x_m \\in X)\nS \\subset \\bigcup_{\\alpha = 1}^m B_\\varepsilon(x_\\alpha)\nwhere B_\\varepsilon is Delta-ball.\nThm 21.1\ntotally bounded \\Longrightarrow bounded"},"RealAnalysis/sequences/uniform-limit":{"slug":"RealAnalysis/sequences/uniform-limit","filePath":"RealAnalysis/sequences/uniform limit.md","title":"uniform limit","links":[],"tags":[],"content":"Def pg. 243\nA sequence of function (f_n) on A \\subseteq \\mathbb{R} to \\mathbb{R} converges uniformly on A_0 \\subseteq A to a function f: A_0 \\longrightarrow \\mathbb{R} if for all x \\in A_0\n(\\forall \\varepsilon &gt; 0)(\\exists K(\\varepsilon))(\\forall n \\geq K(\\varepsilon)) |f_n(x) - f(x)| &lt; \\varepsilon\nClaim 18.2\n\\lim_{n \\longrightarrow \\infty} d_\\infty(f_n, f) = 0"},"RealAnalysis/sequences/uniformly-continue":{"slug":"RealAnalysis/sequences/uniformly-continue","filePath":"RealAnalysis/sequences/uniformly continue.md","title":"uniformly continue","links":["RealAnalysis/sequences/uniformly-continue","RealAnalysis/sequences/interval","RealAnalysis/sequences/continue","RealAnalysis/sequences/Cauchy-sequence","RealAnalysis/sequences/compact","RealAnalysis/sequences/complete"],"tags":[],"content":"Def pg. 143\nlet A \\subseteq \\mathbb{R} and let f: A \\longrightarrow \\mathbb{R}.  We say f is uniformly continue if\n(\\forall \\varepsilon &gt; 0)(\\exists \\delta(\\varepsilon) &gt; 0)(\\forall x,u \\in A) d(x , u) &lt; \\delta(\\varepsilon) \\Longrightarrow d(f(x), f(u)) &lt; \\varepsilon\nThm Uniformly continuity theorem pg. 143\nLet I be a bounded closed interval and let f: I \\longrightarrow \\mathbb{R} be continue on I. Then f is uniformly continue on I.\nThm pg.144\nIf f: A \\longrightarrow \\mathbb{R} is uniformly continue on A \\subseteq \\mathbb{R} and if (x_n) is Cauchy sequence in A, then (f(x_n)) is Cauchy sequence in \\mathbb{R}\nThm 27.1\nFor f: C \\longrightarrow y is continue and C is compact, we can say f is uniformly continue.\nThm 27.2\nFor C compact, y complete, f: D \\longrightarrow y and D \\subset C is dense, TFAE:\n\nF: C \\longrightarrow y continue and F \\mid_D = f(means f is F when domain is restricted to D)\nf is uniformly continue\n"},"NumericalAnalysis/CH6/multi/BFGS-Method":{"slug":"NumericalAnalysis/CH6/multi/BFGS-Method","filePath":"NumericalAnalysis/CH6/multi/BFGS Method.md","title":"BFGS Method","links":["NumericalAnalysis/CH6/multi/BFGS-Method","NumericalAnalysis/CH6/multi/Newton's-method"],"tags":[],"content":"Concept\nBFGS Methodis a secant updating methods of quasi-Newton’s method.\ntake B_0 = I or initial guess of H.\nsolve B_ks_k = -\\nabla f(x_k)\nx_{k + 1} = x_k + s_k\ny_k = \\nabla f(x_{k + 1}) - \\nabla f(x_k)\nB_{k + 1} = B_k + \\frac{y_ky_k^T}{y^T_ks_k} - \\frac{B_ks_ks^T_kB_k}{s^T_kB_ks_k}\nrepeat this."},"NumericalAnalysis/CH6/multi/Conjugate-Gradient-Method":{"slug":"NumericalAnalysis/CH6/multi/Conjugate-Gradient-Method","filePath":"NumericalAnalysis/CH6/multi/Conjugate Gradient Method.md","title":"Conjugate Gradient Method","links":["NumericalAnalysis/CH6/multi/Steepest-Descent-Method","NumericalAnalysis/CH6/multi/Conjugate-Gradient-Method","NumericalAnalysis/CH6/multi/Newton's-method","NumericalAnalysis/CH6/Hessian-matrix","NumericalAnalysis/CH6/gradient"],"tags":[],"content":"Concept\nthe first part is same as Steepest Descent Method, but after find the new x, we calculate next g instead of finding steepest descent by\n\\beta = \\frac{g_1^Tg_1}{g_0^Tg_0}\nand\ns_1 = -g_1 + \\beta s_0\nx0 = initial guess\ng0 = \\nabla f(x0)\ns0 = g0\nfor k = 0, 1, 2,...\n\tChoose αk to minimize f(xk + αk sk)\n\txk+1 = xk + αk sk\n\tgk+1 = \\nabla f(xk+1)\n\tβk+1 = (g^Tk+1 gk+1)/(g^Tk gk)\n\tsk+1 = gk+1 + βk+1sk\nend\n\nConjugate Gradient Method is only different with Steepest Descent Method with \\beta, if we set \\beta constantly equal to 0, it will be same as Steepest Descent Method.\nCost\nunlike Newton’s method, this one doesn’t need to calculate Hessian matrix H nor b, it only need to calculate gradient \\nabla and line search.\n\nCode\nimport numpy as np\nfrom typing import Callable, Tuple\n \ndef backtracking_line_search(  \n\tf: Callable[[np.ndarray], float],  \n\tgrad_f: Callable[[np.ndarray], np.ndarray],  \n\tx: np.ndarray,  \n\td: np.ndarray, # direction, usually - \\nabla grad_f \n\talpha_init: float = 1.0,  \n\trho: float = 0.5, # back trace factor  \n\tc: float = 1e-4 # Armijo cond \n) -&gt; float:\n\talpha = alpha_init  \n\tf_x = f(x)  \n\tgrad_dot_d = np.dot(grad_f(x), d) # \\nabla f(x)^Td  \n\t\n\t# Armijo cond: f(x + α*d) \\leqd f(x) + c*α* \\nabla f(x)^Td  \n\twhile f(x + alpha * d) &gt; f_x + c * alpha * grad_dot_d:  \n\t\talpha *= rho # backtrace\n\t\t \n\treturn alpha\n \ndef conjugate_gradient(\n    f: Callable[[np.ndarray], float],\n    grad_f: Callable[[np.ndarray], np.ndarray],\n    x0: np.ndarray,\n    max_iter: int = 1000,\n    tol: float = 1e-6\n) -&gt; Tuple[np.ndarray, float, int]:\n\tx = x0.copy()\n\tn = len(x)\n\t\n\tg = grad_f(x)\n\td = -g.copy()\n\t\n\titerations = 0\n\t\n\tfor i in range(max_iter):\n\t\tif np.linalg.norm(g) &lt; tol:\n\t\t\tbreak\n\t\t\n\t\talpha = backtracking_line_search(f, grad_f, x, d)\n\t\tx_new = x + alpha * d\n\t\tg_new = grad_f(x_new)\n\t\tbeta = np.dot(g_new, g_new) / np.dot(g, g)\n\t\td = -g_new + beta * d\n\t\tx = x_new\n\t\tg = g_new\n\t\t\n\t\titerations += 1\n\t\t\n\t\t# reset direction\n\t\tif (i + 1) % n == 0:\n\t\t\td = -g.copy()\n\t\t\n\treturn x, f(x), iterations"},"NumericalAnalysis/CH6/multi/Gauss-Newton-Method":{"slug":"NumericalAnalysis/CH6/multi/Gauss-Newton-Method","filePath":"NumericalAnalysis/CH6/multi/Gauss Newton Method.md","title":"Gauss Newton Method","links":["NumericalAnalysis/CH6/multi/Newton's-method","NumericalAnalysis/CH6/Hessian-matrix"],"tags":[],"content":"Concept\nsolve\nJ^T(x_k)J(x_k)s_k = -J^T(x_k)r(x_k)\n\\Longrightarrow J(x_k)s_k = -r(x_k)\ninstead of H_f(x_k)s_k = -\\nabla f(x_k) in Newton’s method.\nIt like Newton’s method applied to truncated Hessian matrix."},"NumericalAnalysis/CH6/multi/Levenberg-Marquardt-method":{"slug":"NumericalAnalysis/CH6/multi/Levenberg-Marquardt-method","filePath":"NumericalAnalysis/CH6/multi/Levenberg-Marquardt method.md","title":"Levenberg-Marquardt method","links":["NumericalAnalysis/CH6/multi/Gauss-Newton-Method","NumericalAnalysis/CH6/multi/Steepest-Descent-Method","NumericalAnalysis/CH5/convergence-rate"],"tags":[],"content":"Concept\nthis method is combination of Gauss Newton Method and Steepest Descent Method:\nsolve\n(J^T_kJ_k + \\mu_kI)s_k = -J_k^Tr_k\nfor s_k where \\mu_k is from Steepest Descent Method.\nCost\nWhen \\mu_k \\to 0 as iterating, this method behaved like Gauss Newton Method(quadratic convergence rate), otherwise it behaved like Steepest Descent Method(linear convergence rate)."},"NumericalAnalysis/CH6/multi/Newton's-method":{"slug":"NumericalAnalysis/CH6/multi/Newton's-method","filePath":"NumericalAnalysis/CH6/multi/Newton's method.md","title":"Newton's method","links":["NumericalAnalysis/CH6/Hessian-matrix","NumericalAnalysis/CH6/gradient","NumericalAnalysis/CH5/convergence-rate","NumericalAnalysis/CH6/multi/Gauss-Newton-Method"],"tags":[],"content":"Concept\nx_{k + 1} = x_k - H^{-1}_f(x_k)\\nabla f(x_k)\nwhere H is Hessian matrix and \\nabla is gradient.\nFor the H^{-1} here, we can solve it instead of inverting like we did in chapter 5:\nH_f(x_k)s_k = -\\nabla f(x_k)\nand\nx_{k + 1} = x_k + s_k\nCost\nit has quadratic convergence rate.\nVariants\n\nGauss Newton Method\n"},"NumericalAnalysis/CH6/multi/Nonlinear-Least-Squares":{"slug":"NumericalAnalysis/CH6/multi/Nonlinear-Least-Squares","filePath":"NumericalAnalysis/CH6/multi/Nonlinear Least Squares.md","title":"Nonlinear Least Squares","links":["NumericalAnalysis/CH5/Jacobian-matrix"],"tags":[],"content":"Concept\nGiven data (t_i , b_i)\ndefine residual function:\nr_i = b_i - f(t_i, x)\nobjective function:\n\\phi(x) = \\frac{1}{2}r^T(x)r(x)\ngradient vector:\n\\nabla\\phi(x) = J^T(x)r(x)\nwhere J is Jacobian matrix of r.\nHessian matrix:\nH_\\phi = J^T(x)J(x) + \\sum^m_{i = 1}r_i(x)H_i(x)\nHowever it’s expensive to expend such sum and since it multiple to r_I which is very small that we can just ignore this whole sum term."},"NumericalAnalysis/CH6/multi/Steepest-Descent-Method":{"slug":"NumericalAnalysis/CH6/multi/Steepest-Descent-Method","filePath":"NumericalAnalysis/CH6/multi/Steepest Descent Method.md","title":"Steepest Descent Method","links":["NumericalAnalysis/CH6/gradient","NumericalAnalysis/CH5/convergence-rate"],"tags":[],"content":"Concept\nx_{k + 1} = x_k - \\alpha_k \\nabla f(x_k)\nwhere a_k is a scalar line search parameter and \\nabla is gradient.\nCost\nlinear convergence rate\nCode\ndef steepest_descent(\n    f: Callable[[np.ndarray], float],\n    grad_f: Callable[[np.ndarray], np.ndarray],\n    x0: np.ndarray,\n    alpha: float = 0.01,  # fixed or line search\n    max_iter: int = 1000,\n    tol: float = 1e-6\n) -&gt; Tuple[np.ndarray, float, int]:\n    x = x0.copy()\n    iterations = 0\n    \n    for i in range(max_iter):\n        g = grad_f(x)\n        \n        if np.linalg.norm(g) &lt; tol:\n            break\n            \n        # x_{k+1} = x_k - α_k \\nabla f(x_k)\n        x = x - alpha * g\n        iterations += 1\n        \n    return x, f(x), iterations\nline search\nimport numpy as np\nfrom typing import Callable, Tuple\n \ndef backtracking_line_search(  \n\tf: Callable[[np.ndarray], float],  \n\tgrad_f: Callable[[np.ndarray], np.ndarray],  \n\tx: np.ndarray,  \n\td: np.ndarray, # direction, usually - \\nabla grad_f \n\talpha_init: float = 1.0,  \n\trho: float = 0.5, # back trace factor  \n\tc: float = 1e-4 # Armijo cond \n) -&gt; float:\n\talpha = alpha_init  \n\tf_x = f(x)  \n\tgrad_dot_d = np.dot(grad_f(x), d) # \\nabla f(x)^Td  \n\t\n\t# Armijo cond: f(x + α*d) \\leqd f(x) + c*α* \\nabla f(x)^Td  \n\twhile f(x + alpha * d) &gt; f_x + c * alpha * grad_dot_d:  \n\t\talpha *= rho # backtrace\n\t\t \n\treturn alpha"},"NumericalAnalysis/CH6/single/Golden-Section-Search":{"slug":"NumericalAnalysis/CH6/single/Golden-Section-Search","filePath":"NumericalAnalysis/CH6/single/Golden Section Search.md","title":"Golden Section Search","links":["NumericalAnalysis/CH6/unimodal","NumericalAnalysis/CH5/convergence-rate"],"tags":[],"content":"Concept\nSupport f is unimodal on [a,b] and let x_1, x_2 two points within [a,b] where x_1 &lt; x_2.\nDiscard [a, x_1) or (x_2, b] depend on f(x_1) &gt; f(x_2) or opposite.\nRepeat this process.\nCost\nconvergence rate is only linear, with constant C \\approx 0.618."},"NumericalAnalysis/CH6/single/Newton’s-method":{"slug":"NumericalAnalysis/CH6/single/Newton’s-method","filePath":"NumericalAnalysis/CH6/single/Newton’s method.md","title":"Newton’s method","links":["NumericalAnalysis/CH5/Newton's-method","NumericalAnalysis/CH5/convergence-rate"],"tags":[],"content":"Concept\nit’s equivalent to find root with CH5 Newton’s method for f&#039;(x).\nx_{k + 1} = x_k - \\frac{f&#039;(x_k)}{f&#039;&#039;(x_k)}\nCost\nIt has quadratic convergence rate if the starter guess is good enough."},"NumericalAnalysis/CH6/single/Successive-Parabolic-Interpolation":{"slug":"NumericalAnalysis/CH6/single/Successive-Parabolic-Interpolation","filePath":"NumericalAnalysis/CH6/single/Successive Parabolic Interpolation.md","title":"Successive Parabolic Interpolation","links":[],"tags":[],"content":""},"RealAnalysis/integrableAndDiff/proofs/Final-Review-problems":{"slug":"RealAnalysis/integrableAndDiff/proofs/Final-Review-problems","filePath":"RealAnalysis/integrableAndDiff/proofs/Final Review problems.md","title":"Final Review problems","links":[],"tags":[],"content":"Radius\nfor f(x) = \\sum k^2 x^2\nR = \\lim_k \\sup \\sqrt[k]{|k^2|} \\Longrightarrow R=1 because\n\nk^{\\frac{2}{k}} \\geq 1 \\Longrightarrow R \\geq 1\nR \\leq 1 because\n(1 + \\varepsilon)^k \\geq (1+\\varepsilon k) \\Longrightarrow (\\exists k_0)(\\forall k \\geq k_0) (1+\\varepsilon)^k \\geq (1+k\\varepsilon) \\geq k^2\n\\square\n\nRiemann integral\nCalculate integral of \\lim_{a\\longrightarrow 1} \\sum cos(a^\\frac{j}{2})a^j(1-a)\nso mesh is like: x_0=1,x_1=a,x_j=a^j,x_N=0\nR_\\infty(f)=\\sum^\\infty_{j=0}f(a^j)(x_j-x_{j + 1}) where x_j - x_{j + 1} = a^j (1 - a)\ntake x_j = a^j = \\xi_j\nso\nR_\\infty(f, \\xi) = \\int^1_0 f(t) dt = \\int^1_0 \\cos(\\sqrt{x})dx\n\\square"},"RealAnalysis/integrableAndDiff/proofs/Problem-of-class-38":{"slug":"RealAnalysis/integrableAndDiff/proofs/Problem-of-class-38","filePath":"RealAnalysis/integrableAndDiff/proofs/Problem of class 38.md","title":"Problem of class 38","links":["RealAnalysis/integrableAndDiff/Power-theory"],"tags":[],"content":"For\nf(z) = \\frac{1}{(z - i)(z + i)}\nconsider f(z) = \\frac{A}{z-i} + \\frac{B}{z + i} and solve A, B\nAnd it’s f(z) = \\frac{-i/2}{z-i} + \\frac{i/2}{z+i}.\nThen do Power theory:\nf(z) = \\frac{1}{2}\\left(\\frac{1}{1 - (\\frac{-z}{i})} + \\frac{1}{1-\\frac{z}{i}}\\right) = \\frac{1}{2}\\left[\\sum^\\infty_{k=0} (\\frac{-z}{i})^k + \\sum^\\infty_{k=0} (\\frac{z}{i})^k\\right]=\\sum^\\infty_{k=0} \\frac{z^{2k}}{i^{2k}}\n\\square"},"RealAnalysis/integrableAndDiff/proofs/Proofs-of-pi":{"slug":"RealAnalysis/integrableAndDiff/proofs/Proofs-of-pi","filePath":"RealAnalysis/integrableAndDiff/proofs/Proofs of pi.md","title":"Proofs of pi","links":[],"tags":[],"content":"pi &gt;= 2\n\\cos(x) - 1 = - \\int^x_0 \\sin(t)dt\n\\Longrightarrow 2 = \\int^\\pi_0 \\sin(t) dt \\leq \\int^\\pi_0 t dt = \\frac{t^2}{2}\\bigg\\vert^\\pi_0 = \\frac{\\pi^2}{2}\n\\Longrightarrow 4 \\leq \\pi^2 \\Longrightarrow 2 \\leq \\pi\n\\square\npi &gt;= 2\\sqrt{2}\nThere is a simple way to do it with graph\n\nSo clearly, 2\\pi \\geq 4\\sqrt{2} \\Longrightarrow \\pi \\geq 2\\sqrt{2}"},"RealAnalysis/sequences/proofs/3-lemmas-of-class-28":{"slug":"RealAnalysis/sequences/proofs/3-lemmas-of-class-28","filePath":"RealAnalysis/sequences/proofs/3 lemmas of class 28.md","title":"3 lemmas of class 28","links":["RealAnalysis/sequences/totally-bounded","RealAnalysis/sequences/bounded"],"tags":[],"content":"Lemma 23.3\nFor set S \\subset X, S is totally bounded \\Longrightarrow S is bounded.\nSuppose S is totally bounded, we have\n(\\forall \\varepsilon &gt; 0)(\\exists m)(\\exists x_1, x_2, \\dots, x_m \\in X) \\bigcup^m_{\\alpha = 1} S \\subset B_\\varepsilon(x_\\alpha)\nand we want to prove that\n(\\exists R &gt; 0)(\\exists x \\in X) S \\subset B_R(x)\nThen take \\varepsilon = 1 and take x = x_\\alpha where \\alpha \\leq m, we can take R = \\max(\\{d(x_\\alpha, x_k) \\mid x_k \\in X \\}) which is the max distance between every points in X.\nAfter that, it’s safe to say S is bounded. \\square\nLemma 23.4\nFor set S \\subset \\mathbb{R}^d, S is bounded \\Longrightarrow S is totally bounded"},"RealAnalysis/sequences/proofs/Practice-of-exam-2":{"slug":"RealAnalysis/sequences/proofs/Practice-of-exam-2","filePath":"RealAnalysis/sequences/proofs/Practice of exam 2.md","title":"Practice of exam 2","links":["RealAnalysis/sequences/closed","RealAnalysis/sequences/compact","RealAnalysis/sequences/bounded"],"tags":[],"content":"2. Closure is closed\nLet (x_n) \\subset \\mathbb{R}. Show that\nS = \\{x_n | n \\in \\mathbb{N}\\} \\cup Lim(x_n)\nis closed. Show that S is compact \\iff (x_n) is bounded.\nFirstly, we can prove S is closed by contradiction: suppose there exists (x_{n_j}) \\subseteq S such that \\lim(x_{n_j}) \\not\\in S, we have two cases:\n\nIf (x_{n_j}) \\subseteq (x_n),  it leads to a contradiction with lim(x_{n_j}) \\not\\in S due to Lim(x_n) \\subset S and \\lim(x_{n_j}) \\in Lim(x_n).\nIf A = (x_{n_j}) \\backslash (x_n) \\neq \\emptyset and \\lim(x_{n_j}) = x&#039;, we can say A \\subset Lim(x_n) and \\lim(x_{n_j}) = \\lim(A) \\in Lim(x_n) which leads to a contradiction with \\lim(x_{n_j}) \\not\\in S.\nHence in both cases we can say S is closed proved by contradiction. \\square\n"},"RealAnalysis/sequences/proofs/Problem-of-OH-after-class-28":{"slug":"RealAnalysis/sequences/proofs/Problem-of-OH-after-class-28","filePath":"RealAnalysis/sequences/proofs/Problem of OH after class 28.md","title":"Problem of OH after class 28","links":["RealAnalysis/sequences/compact","RealAnalysis/sequences/continue"],"tags":[],"content":"For S \\subset X compact and a x \\not\\in S\nShow \\exists \\varepsilon &gt; 0, s.t.\n\\inf_{y \\in S} d(x, y) \\geq \\varepsilon &gt; 0\nThen according to d(x, y) is continue, we can say f(y) = -d(x, y) is also continue.\nAfter that we can say\n(\\exists y_0) \\sup_{y \\in S} f(y) = f(y_0)\nis true by Thm 28 OH. According to x \\not\\in S, we can say f(y_0) \\neq 0 is true.\nHence we can take \\varepsilon = d(x, y_0) and\n\\inf_{y \\in S} d(x, y) \\geq \\varepsilon = d(x, y_0) &gt; 0\nis true. \\square"},"RealAnalysis/sequences/proofs/proof-of-Thm-15.1":{"slug":"RealAnalysis/sequences/proofs/proof-of-Thm-15.1","filePath":"RealAnalysis/sequences/proofs/proof of Thm 15.1.md","title":"proof of Thm 15.1","links":["RealAnalysis/sequences/complete","RealAnalysis/sequences/Metric-Space","RealAnalysis/sequences/closed","RealAnalysis/sequences/Cauchy-sequence","RealAnalysis/sequences/sequentially-Closed"],"tags":[],"content":"Thm 15.1\nFor X(C, d) a Metric Space\nproof 1 → 2\nSuppose C is closed, we want to prove X is complete.\nConsider (x_n) \\subseteq C as an arbitrary Cauchy sequence in C, according to C is closed, which imply C is sequentially Closed by Prop 15.2, we can say\n\\lim_n (x_n) = x&#039; \\Longrightarrow x&#039; \\in (x_n)\nis true. Hence it’s safe to say X is complete. \\square\nproof 2 → 1\nSuppose X is complete, we want to prove C is closed."},"RealAnalysis/sequences/proofs/proof-of-Thm-21.7":{"slug":"RealAnalysis/sequences/proofs/proof-of-Thm-21.7","filePath":"RealAnalysis/sequences/proofs/proof of Thm 21.7.md","title":"proof of Thm 21.7","links":["RealAnalysis/sequences/compact","RealAnalysis/sequences/bounded","RealAnalysis/sequences/open-set","RealAnalysis/sequences/subcover"],"tags":[],"content":"Compact - Thm 21.7\nProof 1 → 2\nFor compact set K \\subseteq \\mathbb{R},\nBounded\nwe can prove it’s bounded at first by:\nConsider H_m = (-m, m) as an open set, then according to\nK \\subseteq \\bigcup_{m = 1}^\\infty H_m = \\mathbb{R}\nwe can say H_m is an subcover of K. According to K is compact, we can say (H_m) is finite. Then we have\nK \\subseteq \\bigcup_{m = 1}^M H_m = (-M, M)\nHence, we can say K is bounded by (-M, M).\n\\square\nClosed\nProof 3 → 2"}}